{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "92c137b4aad3d4dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# <font color=\"dodgerblue\">YOLO-Distance Model Fine-Tuning for Normal and Blurred Datasets</font>\n",
    "\n",
    "**Author:** Teague Sangster\n",
    "**Date Updated:** May 12, 2025\n",
    "**Project Goal:** This Jupyter Notebook orchestrates our fine-tuning of two YOLO-Distance models! One model is trained on a standard (\"tuned\") image dataset, and the other is trained on a version of the same dataset where a depth-of-field blur has been programmatically applied. The objective is to compare model performance which is done in the seperate compare file (Also on Github). The process starts both models from the same initial pre-trained weights and alternates their training in sessions. \n",
    "\n",
    "I wrote summarys at the top of each block so please read those for more insight. \n",
    "\n",
    "---\n",
    "\n",
    "### <font color=\"teal\">How it all works!</font>\n",
    "\n",
    "\n",
    "1.  **<font color=\"darkorange\">Cell 1: Initial Setup, Imports, and Core Paths</font>**\n",
    "    * **Environment Setup:** Imports essential Python libraries (`sys`, `os`, `numpy`, `tensorflow`, `argparse`, `time`, `shutil`, `datetime`). TensorFlow is specifically chosen for YOLO v3 compatibility and potential hardware acceleration (MPS on Mac).\n",
    "    * **Path Modification:** Adds the path to a cloned `yolo-with-distance` repository to `sys.path`.\n",
    "    * **Import Verification:** Confirms that custom modules (like `get_classes`) can be successfully imported.\n",
    "    * **Path Definitions:** Establishes all crucial paths for initial model weights, directories for saving new model runs (separate for \"tuned\" and \"blurred\" models), image data, label files, and logging directories.\n",
    "    * **Directory Creation:** Ensures all specified output and log directories are created.\n",
    "    * **YOLO & Training Parameters:** Defines common YOLO model parameters and master training control parameters like `MAX_TOTAL_TRAINING_TIME_SECONDS`, `BREAK_TIME_SECONDS`, and `EPOCHS_PER_MAIN_SESSION`.\n",
    "\n",
    "2.  **<font color=\"darkorange\">Cell 2: Helper Function - `create_aggregated_annotation_file`</font>**\n",
    "    * **Purpose:** Combines individual KITTI-style label files into a single aggregated annotation file for YOLO training.\n",
    "    * **Functionality:** Loads class names, maps them to IDs, iterates through label files, matches them to images, parses object data (bounding boxes, class IDs, **distance**), and formats this into an output file.\n",
    "    * *Note: The project reuses labels for modified images, but separate labels are advised for distinct datasets.*\n",
    "\n",
    "3.  **<font color=\"darkorange\">Cell 3: (Assumed) Annotation File Generation</font>**\n",
    "    * *(This cell likely calls `create_aggregated_annotation_file` from Cell 2 for both \"tuned\" and \"blurred\" datasets, producing their respective annotation `.txt` files.)*\n",
    "\n",
    "4.  **<font color=\"darkorange\">Cell 4: Helper Function - `run_training_session` (Revised)</font>**\n",
    "    * **Purpose:** Manages and executes a single training session for either model, with safety and checkpointing.\n",
    "    * **Safety Callbacks (Keras):**\n",
    "        * `LossGuardCallback`: Flags epochs with loss > 50 as potentially invalid.\n",
    "        * `EmergencyStopCallback`: Stops training if loss hits a critical threshold (e.g., 1000).\n",
    "    * **Log/Checkpoint Helpers:**\n",
    "        * `get_latest_timestamped_log_subdir`: Finds the most recent log subdirectory.\n",
    "        * `find_best_epoch_checkpoint_in_session_log_dir`: Scans for the best checkpoint (`.h5` file) by lowest validation loss, filtering high-loss files.\n",
    "    * **`run_training_session` Functionality:** Sets up model-specific parameters, loads weights, constructs arguments for the external `train.py` script (disabling `eval_online`), configures GPU, calls the training script, and then copies the best session checkpoint to the overall best model path if no loss explosion is detected in the filename.\n",
    "\n",
    "5.  **<font color=\"darkorange\">Cell 5: Quick Test Run Phase</font>**\n",
    "    * **Purpose:** Executes a brief (e.g., 1-epoch) \"smoke test\" for both \"tuned\" and \"blurred\" models.\n",
    "    * **Functionality:** Calls `run_training_session` for each model with minimal epochs, using dedicated test log/model paths to verify the pipeline before full training. Includes a break for resource management.\n",
    "\n",
    "6.  **<font color=\"darkorange\">Cell 6: Main Alternating Training Loop</font>**\n",
    "    * **Purpose:** The primary execution block that iteratively trains the \"tuned\" and \"blurred\" models in alternating sessions.\n",
    "    * **Functionality:** Initializes trackers for epochs and validation losses. Loops while `training_active` and within `MAX_TOTAL_TRAINING_TIME_SECONDS`. In each iteration, it sets up parameters for the `current_model_turn`, calls `run_training_session`, updates total epochs and best validation loss, switches models, and takes a `BREAK_TIME_SECONDS` pause. A `finally` block provides a concluding summary.\n"
   ],
   "id": "e1dde15133881928"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <font color=\"dodgerblue\">Cell 1: Initial Setup, Imports, and Core Paths</font>\n",
    "\n",
    "This cell focuses on setting up the Python environment and defining essential file paths and parameters for our **YOLO (You Only Look Once) - Distance model**. We have two models we are training, but here we are going to start with the *same weights for both* so they have the same start point.\n",
    "\n",
    "**<font color=\"teal\">Imports:</font>**\n",
    "* `sys` and `os`: For system and operating system interactions (like path manipulation and directory creation).\n",
    "* `numpy`: For numerical operations.\n",
    "* `tensorflow`: For deep learning tasks (likely the backend or framework for the YOLO model). <font color=\"chocolate\">The main reason we chose YOLO v3 is the TensorFlow dependency versions ensured I could hardware accelerate training on my Laptop. I know it's not ideal but when you have 2 days to finish a model you do what you can do.</font>\n",
    "* `argparse`: For creating argument namespaces, suggesting this script might interact with or prepare arguments for another script like `train.py`.\n",
    "* `time`: For time-related functions (e.g., pausing).\n",
    "* `shutil`: For file operations like copying.\n",
    "* `datetime`: <font color=\"chocolate\">Used to time how long each pass takes and to stop training after 9 hours. This file was run a few times with different changes to the `train.py` file so having a timer was key to running it overnight and while I was out.</font>\n",
    "\n",
    "**<font color=\"teal\">Python Path Modification:</font>**\n",
    "* It adds a specified repository path (`/Users/teaguesangster/Code/Python/ComputerVisionFinal/ExistingModel/yolo-with-distance/`) to the `sys.path`.\n",
    "* <font color=\"chocolate\">This is done to allow the script to import custom modules that aren't directly in this section of the repository. Originally I was using the `Train.py` file and such provided by https://gitlab.com/EnginCZ/yolo-with-distance. However to optimize this for Mac those files had to be edited as well. Regardless that path points to where that directory is cloned.</font>\n",
    "\n",
    "**<font color=\"teal\">Import Verification is in ensuring everything gets loaded from those paths:</font>**\n",
    "* It attempts to import `get_classes` from `common.utils` (a module within the appended repository path). This acts as a check to ensure the path modification was successful and the necessary custom code is accessible. If the import fails, an error message is printed, and the script exits.\n",
    "\n",
    "**<font color=\"teal\">Core Path Definitions:</font>**\n",
    "* It defines several string variables for crucial file and directory paths:\n",
    "    * `initial_weights_path`: Path to pre-trained model weights (`trained_final_original.h5`). <font color=\"chocolate\">These weights are the starting point for both models. These are graciously provided by the earlier linked GitLab. We couldn't have done this project without their help, thank you!</font>\n",
    "    * `runs_base_path`: A base directory for storing model training runs.\n",
    "    * `tuned_model_save_dir` and `blurred_model_save_dir`: Subdirectories within `runs_base_path` for saving models trained on tuned (normal) and blurred datasets, respectively.\n",
    "    * `tuned_model_best_h5_path` and `blurred_model_best_h5_path`: Specific paths for the best saved `.h5` model files for each dataset type. <font color=\"chocolate\">These are our checkpoints as we had runaway errors with our weights in the past.</font>\n",
    "    * `normal_images_base_path`, `blurred_images_base_path`, `labels_base_path`: Paths to the directories containing normal images, pre-blurred images, and their corresponding label files.\n",
    "    * `log_dir_base_tuned`, `log_dir_base_blurred`, and their `_test` counterparts: Base directories for storing training logs. The script anticipates that a training script (`train.py`) will create timestamped subfolders within these.\n",
    "\n",
    "**<font color=\"teal\">Directory Creation:</font>**\n",
    "* It iterates through a list of the defined path variables and uses `os.makedirs(path, exist_ok=True)` to ensure all these directories exist, creating them if they don't.\n",
    "\n",
    "**<font color=\"teal\">Common YOLO Parameters:</font>**\n",
    "* A dictionary `common_yolo_params` is created to store shared configuration parameters for the YOLO model. This includes:\n",
    "    * `model_type` (e.g., 'yolo3_xception').\n",
    "    * Paths to anchor and class definition files.\n",
    "    * `model_image_size` (e.g., (608, 608)).\n",
    "    * `elim_grid_sense` (a boolean training parameter).\n",
    "\n",
    "**<font color=\"teal\">Training Control Parameters:</font>**\n",
    "Note THERE IS A 2 MIN TIME BUFFER FOR SWITHCING WHICH MODEL IS TRAINING, this was to minitmize memeory compression on my laptop as it uses shared memeory and i've had projects fail training seesons in the past due to model changes.\n",
    "* It defines several constants to control the training process:\n",
    "    * `MAX_TOTAL_TRAINING_TIME_SECONDS`: Maximum duration for the entire training.\n",
    "    * `BREAK_TIME_SECONDS`: Pause duration between switching models or training sessions.\n",
    "    * `EPOCHS_PER_MAIN_SESSION`: Number of epochs to train each model for in a primary training loop.\n",
    "    * `BASE_EPOCHS_ORIGINAL_MODEL`: The number of epochs the initial weights were trained for.\n",
    "\n",
    "**<font color=\"teal\">Print Statements:</font>**\n",
    "* The cell concludes by printing several of the defined paths and control parameters for verification and then a \"Cell 1: Setup Complete ---\" message."
   ],
   "id": "aa2d607c570d49f2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-05-14T01:09:07.490678Z",
     "start_time": "2025-05-14T01:09:05.387436Z"
    }
   },
   "source": [
    "# Cell 1: Initial Setup, Imports, and Core Paths\n",
    "# This cell focuses on setting up the Python environment and defining essential\n",
    "# file paths and parameters for our YOLO (You Only Look Once) - Distance model.\n",
    "# We have two models we are training but here we are going to start with the\n",
    "# same weights for both so they have the same start point.\n",
    "\n",
    "import sys # For system-specific parameters and functions, like modifying the Python path.\n",
    "import os # For interacting with the operating system (e.g., file paths, directory creation).\n",
    "import numpy as np # For numerical operations, especially with arrays.\n",
    "# from PIL import Image # Imported by data.py, may not be directly needed here (commented out as per original)\n",
    "import tensorflow as tf # For deep learning tasks; YOLO v3 was chosen for TensorFlow dependency versions to ensure hardware acceleration on a Mac.\n",
    "# import matplotlib.pyplot as plt # Optional: for any inline plotting/debugging (commented out as per original)\n",
    "import argparse # For creating argument namespaces, useful if this script prepares args for train.py.\n",
    "import time # For time-related functions (e.g., pausing).\n",
    "import shutil # For file operations like copying.\n",
    "from datetime import datetime # Used to time how long each pass takes and to stop training after a set duration.\n",
    "\n",
    "print(\"--- Cell 1: Initial Setup ---\")\n",
    "\n",
    "# Add the cloned repository directory to the Python path\n",
    "# This allows importing custom modules from that specific repository.\n",
    "# Originally from https://gitlab.com/EnginCZ/yolo-with-distance, but files were edited for Mac optimization.\n",
    "repo_path = '/Users/teaguesangster/Code/Python/ComputerVisionFinal/ExistingModel/yolo-with-distance/'\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "    print(f\"Appended to sys.path: {repo_path}\")\n",
    "else:\n",
    "    print(f\"{repo_path} already in sys.path.\")\n",
    "\n",
    "# Attempt to import a module from the repo to verify path\n",
    "# This acts as a check to ensure the path modification was successful.\n",
    "try:\n",
    "    from common.utils import get_classes # Used in annotation file creation from the yolo-with-distance repo.\n",
    "    print(\"Successfully imported 'get_classes' from common.utils.\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"ERROR: Could not import 'get_classes'. ModuleNotFoundError: {e}\")\n",
    "    sys.exit(\"Critical import failed. Ensure common.utils is accessible via repo_path.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR: Could not import 'get_classes' due to ImportError: {e}\")\n",
    "    sys.exit(\"Critical import failed. Check dependencies or conflicts.\")\n",
    "\n",
    "\n",
    "# --- Core Paths ---\n",
    "# Defines several string variables for crucial file and directory paths.\n",
    "initial_weights_path = os.path.join(repo_path, 'weights', 'trained_final_original.h5') # Path to pre-trained model weights (starting point for both models, provided by EnginCZ).\n",
    "runs_base_path = '/Users/teaguesangster/Code/Python/ComputerVisionFinal/runs' # A base directory for storing model training runs.\n",
    "\n",
    "# Subdirectories for saving models trained on tuned (normal) and blurred datasets.\n",
    "tuned_model_save_dir = os.path.join(runs_base_path, \"tuned\")\n",
    "blurred_model_save_dir = os.path.join(runs_base_path, \"blurred\")\n",
    "\n",
    "# Specific paths for the best saved .h5 model files (checkpoints).\n",
    "tuned_model_best_h5_path = os.path.join(tuned_model_save_dir, \"yolo-distance-tuned_best.h5\")\n",
    "blurred_model_best_h5_path = os.path.join(blurred_model_save_dir, \"yolo-distance-blured_best.h5\")\n",
    "\n",
    "# Paths to the directories containing image and label data.\n",
    "normal_images_base_path = \"/Users/teaguesangster/Code/Python/ComputerVisionFinal/TrainingData/data_object_image_2/training/image_2/\"\n",
    "blurred_images_base_path = \"/Users/teaguesangster/Code/Python/ComputerVisionFinal/TrainingData/BlurredImages/BlurredTrainingData/\"\n",
    "labels_base_path = \"/Users/teaguesangster/Code/Python/ComputerVisionFinal/TrainingData/training/label_2/\"\n",
    "\n",
    "# Base log directories (train.py will create timestamped subfolders within these).\n",
    "log_dir_base_tuned = os.path.join(repo_path, 'logs_finetune_tuned')\n",
    "log_dir_base_blurred = os.path.join(repo_path, 'logs_finetune_blurred')\n",
    "test_log_dir_base_tuned = log_dir_base_tuned + \"_test\" # For quick test runs\n",
    "test_log_dir_base_blurred = log_dir_base_blurred + \"_test\"\n",
    "\n",
    "# List of paths to ensure existence (will be created if they don't exist).\n",
    "paths_to_create = [\n",
    "    runs_base_path, tuned_model_save_dir, blurred_model_save_dir,\n",
    "    log_dir_base_tuned, log_dir_base_blurred,\n",
    "    test_log_dir_base_tuned, test_log_dir_base_blurred\n",
    "]\n",
    "for path_to_create in paths_to_create: # Iterate and create directories.\n",
    "    os.makedirs(path_to_create, exist_ok=True) # exist_ok=True prevents error if directory already exists.\n",
    "    # print(f\"Ensured directory exists: {path_to_create}\")\n",
    "\n",
    "# --- Common YOLO Parameters (for train.py and annotation generation) ---\n",
    "# A dictionary to store shared configuration parameters for the YOLO model.\n",
    "common_yolo_params = {\n",
    "    \"model_type\": 'yolo3_xception', # Specifies the YOLO model architecture.\n",
    "    \"anchors_path\": os.path.join(repo_path, 'configs', 'yolo3_anchors.txt'), # Path to anchor definitions.\n",
    "    \"classes_path\": os.path.join(repo_path, 'configs', 'kitty_all_except_nodata.txt'), # Path to class definitions.\n",
    "    \"model_image_size\": (608, 608), # Input image size for the model.\n",
    "    \"elim_grid_sense\": True # A boolean training parameter from the original notebook.\n",
    "}\n",
    "# print(f\"Common YOLO parameters: {common_yolo_params}\")\n",
    "\n",
    "# --- Training Control Parameters ---\n",
    "# Defines several constants to control the training process.\n",
    "MAX_TOTAL_TRAINING_TIME_SECONDS = 9 * 3600 # Maximum duration (9 hours) for the entire training (used for overnight runs).\n",
    "BREAK_TIME_SECONDS = 120 # Pause duration (2 minutes) between switching models or training sessions.\n",
    "EPOCHS_PER_MAIN_SESSION = 5 # Number of epochs to train each model for in a primary training loop.\n",
    "BASE_EPOCHS_ORIGINAL_MODEL = 43 # The number of epochs the initial_weights_path model was originally trained for.\n",
    "\n",
    "# Print some of the defined paths and control parameters for verification.\n",
    "print(f\"Initial weights path: {initial_weights_path}\")\n",
    "print(f\"Max training time: {MAX_TOTAL_TRAINING_TIME_SECONDS / 3600:.2f} hours\")\n",
    "print(f\"Break time between model switches: {BREAK_TIME_SECONDS} seconds\")\n",
    "print(f\"Epochs per main training session: {EPOCHS_PER_MAIN_SESSION}\")\n",
    "print(f\"Base epochs of original model: {BASE_EPOCHS_ORIGINAL_MODEL}\")\n",
    "print(\"--- Cell 1: Setup Complete ---\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Initial Setup ---\n",
      "Appended to sys.path: /Users/teaguesangster/Code/Python/ComputerVisionFinal/ExistingModel/yolo-with-distance/\n",
      "Successfully imported 'get_classes' from common.utils.\n",
      "Initial weights path: /Users/teaguesangster/Code/Python/ComputerVisionFinal/ExistingModel/yolo-with-distance/weights/trained_final_original.h5\n",
      "Max training time: 9.00 hours\n",
      "Break time between model switches: 120 seconds\n",
      "Epochs per main training session: 5\n",
      "Base epochs of original model: 43\n",
      "--- Cell 1: Setup Complete ---\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <font color=\"dodgerblue\">Cell 2: Helper Function - Create Aggregated Annotation File</font>\n",
    "\n",
    "Here we define the `create_aggregated_annotation_file` which handles combining all of the image labels for both of our datasets. Here we are using the *same labels for both of our datasets* (this is because we are training off modifications made to the images). Aggregating all of our KITTI-style label files, we create a corresponding directory list for all the images we wish to train on. <font color=\"chocolate\">Word of advice! If you want to use this code make sure you make separate label files if you have separate datasets.</font>\n",
    "\n",
    "**Here's a breakdown of its functionality:**\n",
    "Here we take a few steps:\n",
    "\n",
    "1.  **<font color=\"teal\">Load Class Names</font>**\n",
    "    * Class names here actually don't come from the individual label files; they come from a separate definition in Cell 1 (`model_classes_file_path` which points to `kitty_all_except_nodata.txt` via `common_yolo_params`).\n",
    "    * <font color=\"chocolate\">We do this as we are folding some classes into each other to simplify training. I'm pretty sure this is why our initial code run lost all of its confidence. We didn't freeze most of our layers and as such, we started removing already trained classes by folding labels in on themselves.</font>\n",
    "\n",
    "2.  **<font color=\"teal\">Create Class ID Map</font>**\n",
    "    * It then creates a mapping (`name_to_id_map`) from these class names to integer class IDs, which is necessary for formatting the annotations in the way YOLO expects.\n",
    "\n",
    "3.  **<font color=\"teal\">Directory Checks</font>**\n",
    "    * It checks if the provided `label_folder` and `image_folder` exist. If either is not found, it prints an error and returns, preventing further execution.\n",
    "    * <font color=\"chocolate\">If you are running this in Colab, you have to add an import drive function before this.</font>\n",
    "\n",
    "**<font color=\"teal\">We then iterate through all label files:</font>**\n",
    "* It lists all `.txt` files (label files) in the `label_folder` and sorts them for consistent processing order. If no label files are found, it prints an error and returns.\n",
    "* For each label file:\n",
    "    * It determines the corresponding image filename (by replacing `.txt` with `.png`). <font color=\"chocolate\">Again, we have 2 datasets here using the same labels, so be careful!</font>\n",
    "    * It constructs the absolute path to the image file and checks if this image file *actually exists* in the `image_folder`. If not, it increments a `skipped_images_count` and moves to the next label file.\n",
    "    * It initializes a list `line_parts_for_image` starting with the absolute path to the current image file. This list will eventually form one line in the output annotation file.\n",
    "\n",
    "    **<font color=\"darkorange\">Parse Individual Label Files:</font>**\n",
    "    * It opens the current KITTI label file.\n",
    "    * For each line (representing an object) in the KITTI label file:\n",
    "        * It splits the line into parts.\n",
    "        * It extracts the ground truth class name.\n",
    "        * It checks if the class name is 'DontCare' or if it's not present in the `name_to_id_map` (meaning it's not a class the model is being trained on). If so, the object is skipped.\n",
    "        * If the object is relevant, it attempts to parse:\n",
    "            * The 2D bounding box coordinates (`xmin`, `ymin`, `xmax`, `ymax`).\n",
    "            * <font color=\"chocolate\">The most important thing here for our training was the **distance** of the object (from the 14th element, `obj_parts[13]`, which is the z-coordinate in KITTI). We load this value alongside all the expected values for a YOLO dataset.</font>\n",
    "        * We then format all of our information into a string: `\"int(xmin),int(ymin),int(xmax),int(ymax),class_id_model,distance:.6f\"`. The distance is formatted as a float with 6 decimal places. This string is appended to `line_parts_for_image`.\n",
    "        * Error handling (a `try-except` block) is included to catch issues during the parsing of individual object lines (e.g., `ValueError`, `KeyError`, `IndexError`) and prints a warning if an object is skipped.\n",
    "\n",
    "**<font color=\"teal\">Aggregate Annotations:</font>**\n",
    "* If `line_parts_for_image` contains more than just the image path (meaning at least one valid object was found for that image), the parts are joined into a single string with spaces and appended to the `aggregated_annotations` list.\n",
    "\n",
    "**<font color=\"teal\">Report Skipped Images:</font>**\n",
    "* If any images were skipped because their files weren't found, a warning message is printed.\n",
    "\n",
    "**<font color=\"teal\">Write Output File:</font>**\n",
    "* Finally, it opens the specified `output_annotation_file_path` in write mode.\n",
    "* It writes each line from `aggregated_annotations` to this file, followed by a newline character.\n",
    "* It prints a confirmation message indicating where the aggregated file was created and how many image entries it contains.\n",
    "\n",
    "*The cell also includes print statements at the beginning and end to indicate the definition phase of this function.*"
   ],
   "id": "ed383cf5ba5e7d3b"
  },
  {
   "cell_type": "markdown",
   "id": "10a4061e302d5f84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "76ee087eb81523df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T01:09:07.497506Z",
     "start_time": "2025-05-14T01:09:07.491546Z"
    }
   },
   "source": [
    "# Cell 2: Helper Function - Create Aggregated Annotation File\n",
    "# Here we define the create_aggregated_annotation_file which handles combining all of the\n",
    "# image labels for both of our datasets. We are using the same labels for both datasets\n",
    "# because we are training off modifications made to the images.\n",
    "# Word of advice! If you want to use this code make sure you make separate label files\n",
    "# if you have separate datasets.\n",
    "print(\"--- Cell 2: Defining create_aggregated_annotation_file function ---\")\n",
    "\n",
    "def create_aggregated_annotation_file(image_folder, label_folder, model_classes_file_path, output_annotation_file_path):\n",
    "    \"\"\"\n",
    "    Aggregates KITTI-style label files into a single annotation file for YOLO training.\n",
    "    Combines individual label files with their corresponding image paths and formats\n",
    "    object data (bbox, class_id, distance) as required by the training script.\n",
    "    \"\"\"\n",
    "    # 1. Load Class names\n",
    "    # Class names here actually don't come from the individual label files; they come from\n",
    "    # a separate definition in Cell 1 (model_classes_file_path which points to\n",
    "    # kitty_all_except_nodata.txt via common_yolo_params).\n",
    "    # We do this as we are folding some classes into each other to simplify training.\n",
    "    # I'm pretty sure this is why our initial code run lost all of its confidence.\n",
    "    # We didn't freeze most of our layers and as such, we started removing already\n",
    "    # trained classes by folding labels in on themselves.\n",
    "    print(f\"Attempting to load model class names from: {model_classes_file_path}\")\n",
    "    model_class_names_list = get_classes(model_classes_file_path) # Uses imported get_classes from common.utils\n",
    "    # 2. It then creates a mapping (name_to_id_map) from these class names to integer class IDs,\n",
    "    # which is necessary for formatting the annotations.\n",
    "    name_to_id_map = {name: i for i, name in enumerate(model_class_names_list)}\n",
    "    # print(f\"Model class map for annotation generation: {name_to_id_map}\")\n",
    "\n",
    "    aggregated_annotations = [] # List to store each line of the final annotation file.\n",
    "\n",
    "    # 3. Directory Checks:\n",
    "    # It checks if the provided label_folder and image_folder exist.\n",
    "    # If you are running this in Colab you have to add an import drive function before.\n",
    "    if not os.path.isdir(label_folder):\n",
    "        print(f\"ERROR: Label folder not found at {label_folder}\")\n",
    "        return\n",
    "    if not os.path.isdir(image_folder):\n",
    "        print(f\"ERROR: Image folder not found at {image_folder}\")\n",
    "        return\n",
    "\n",
    "    # We then iterate through all label files ->\n",
    "    # It lists all .txt files (label files) in the label_folder and sorts them\n",
    "    # for consistent processing order.\n",
    "    label_filenames = sorted([f for f in os.listdir(label_folder) if f.endswith('.txt')])\n",
    "    if not label_filenames:\n",
    "        print(f\"Error: No label files (.txt) found in {label_folder}\")\n",
    "        return\n",
    "    print(f\"Found {len(label_filenames)} label files in {label_folder} for {os.path.basename(image_folder)}. Processing...\")\n",
    "\n",
    "    skipped_images_count = 0 # Counter for images whose files are not found.\n",
    "    # For each label file:\n",
    "    for label_filename in label_filenames:\n",
    "        # It determines the corresponding image filename (by replacing .txt with .png).\n",
    "        # Again we have 2 datasets here using the same labels so be careful!\n",
    "        image_filename = label_filename.replace('.txt', '.png')\n",
    "        # It constructs the absolute path to the image file.\n",
    "        image_file_path_abs = os.path.join(image_folder, image_filename)\n",
    "        \n",
    "        # and checks if this image file actually exists in the image_folder.\n",
    "        # If not, it increments a skipped_images_count and moves to the next label file.\n",
    "        if not os.path.exists(image_file_path_abs):\n",
    "            skipped_images_count += 1\n",
    "            continue # Skip this label file if the corresponding image doesn't exist.\n",
    "\n",
    "        kitti_label_file_path = os.path.join(label_folder, label_filename)\n",
    "        # It initializes a list line_parts_for_image starting with the absolute path\n",
    "        # to the current image file. This list will eventually form one line\n",
    "        # in the output annotation file.\n",
    "        line_parts_for_image = [image_file_path_abs]\n",
    "\n",
    "        # Parse Individual Label Files:\n",
    "        # It opens the current KITTI label file.\n",
    "        with open(kitti_label_file_path, 'r') as f_label:\n",
    "            # For each line (representing an object) in the KITTI label file:\n",
    "            for kitti_line in f_label:\n",
    "                obj_parts = kitti_line.strip().split() # It splits the line into parts.\n",
    "                # Basic validation of the line format\n",
    "                if not obj_parts or len(obj_parts) < 14: continue\n",
    "                # It extracts the ground truth class name.\n",
    "                class_name_gt = obj_parts[0]\n",
    "                # It checks if the class name is 'DontCare' or if it's not present in the\n",
    "                # name_to_id_map (meaning it's not a class the model is being trained on).\n",
    "                # If so, the object is skipped.\n",
    "                if class_name_gt == 'DontCare' or class_name_gt not in name_to_id_map: continue\n",
    "                \n",
    "                # If the object is relevant, it attempts to parse:\n",
    "                try:\n",
    "                    # The 2D bounding box coordinates (xmin, ymin, xmax, ymax).\n",
    "                    xmin, ymin, xmax, ymax = float(obj_parts[4]), float(obj_parts[5]), float(obj_parts[6]), float(obj_parts[7])\n",
    "                    # The most important thing here for our training was the distance of the object\n",
    "                    # (from the 14th element, obj_parts[13], which is the z-coordinate in KITTI).\n",
    "                    # We load this value alongside all the expected values for a YOLO dataset.\n",
    "                    distance = float(obj_parts[13])\n",
    "                    class_id_model = name_to_id_map[class_name_gt] # Get the integer class ID.\n",
    "                    \n",
    "                    # We then format all of our information into a string:\n",
    "                    # \"int(xmin),int(ymin),int(xmax),int(ymax),class_id_model,distance:.6f\".\n",
    "                    # The distance is formatted as a float with 6 decimal places. This string is appended to line_parts_for_image.\n",
    "                    # Ensuring distance is formatted as float, as data.py's get_ground_truth_data expects float for distance\n",
    "                    box_info_str = f\"{int(xmin)},{int(ymin)},{int(xmax)},{int(ymax)},{class_id_model},{distance:.6f}\" # Keep precision for distance\n",
    "                    line_parts_for_image.append(box_info_str)\n",
    "                # Error handling (try-except block) is included to catch issues during the\n",
    "                # parsing of individual object lines (e.g., ValueError, KeyError, IndexError)\n",
    "                # and prints a warning if an object is skipped.\n",
    "                except (ValueError, KeyError, IndexError) as e:\n",
    "                    print(f\"Warning: Skipping object in {label_filename} for image {image_filename} due to parsing error: '{kitti_line.strip()}' | Error: {e}\")\n",
    "        \n",
    "        # Aggregate Annotations:\n",
    "        # If line_parts_for_image contains more than just the image path (meaning at\n",
    "        # least one valid object was found for that image), the parts are joined into\n",
    "        # a single string with spaces and appended to the aggregated_annotations list.\n",
    "        if len(line_parts_for_image) > 1: # Only add if there are objects for this image.\n",
    "            aggregated_annotations.append(\" \".join(line_parts_for_image))\n",
    "\n",
    "    # Report Skipped Images:\n",
    "    # If any images were skipped because their files weren't found, a warning message is printed.\n",
    "    if skipped_images_count > 0:\n",
    "        print(f\"Warning: Skipped {skipped_images_count} entries because corresponding image files were not found in {image_folder}.\")\n",
    "\n",
    "    # Write Output File:\n",
    "    # Finally, it opens the specified output_annotation_file_path in write mode.\n",
    "    with open(output_annotation_file_path, 'w') as f_out:\n",
    "        # It writes each line from aggregated_annotations to this file, followed by a newline character.\n",
    "        for line in aggregated_annotations:\n",
    "            f_out.write(line + \"\\n\")\n",
    "            \n",
    "    # It prints a confirmation message indicating where the aggregated file was created\n",
    "    # and how many image entries it contains.\n",
    "    print(f\"Aggregated annotation file created at: {output_annotation_file_path} with {len(aggregated_annotations)} image entries.\")\n",
    "\n",
    "# The cell also includes print statements at the beginning and end to indicate the\n",
    "# definition phase of this function.\n",
    "print(\"--- Cell 2: Definition Complete ---\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 2: Defining create_aggregated_annotation_file function ---\n",
      "--- Cell 2: Definition Complete ---\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "7fdcd3a5288391ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "366c8b6dc006110c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T01:09:08.476613Z",
     "start_time": "2025-05-14T01:09:07.498143Z"
    }
   },
   "source": [
    "# Cell 3: Generate Annotation Files for Both Datasets\n",
    "print(\"--- Cell 3: Generating Annotation Files ---\")\n",
    "\n",
    "annotation_file_tuned = os.path.join(repo_path, \"kitti_train_list_for_keras_yolo_tuned.txt\")\n",
    "print(f\"\\nCreating annotation file for yolo-distance-tuned using images from: {normal_images_base_path}\")\n",
    "create_aggregated_annotation_file(\n",
    "    image_folder=normal_images_base_path,\n",
    "    label_folder=labels_base_path,\n",
    "    model_classes_file_path=common_yolo_params['classes_path'],\n",
    "    output_annotation_file_path=annotation_file_tuned\n",
    ")\n",
    "\n",
    "annotation_file_blurred = os.path.join(repo_path, \"kitti_train_list_for_keras_yolo_blured.txt\")\n",
    "print(f\"\\nCreating annotation file for yolo-distance-blured using images from: {blurred_images_base_path}\")\n",
    "create_aggregated_annotation_file(\n",
    "    image_folder=blurred_images_base_path,\n",
    "    label_folder=labels_base_path,\n",
    "    model_classes_file_path=common_yolo_params['classes_path'],\n",
    "    output_annotation_file_path=annotation_file_blurred\n",
    ")\n",
    "print(\"--- Cell 3: Annotation File Generation Complete ---\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 3: Generating Annotation Files ---\n",
      "\n",
      "Creating annotation file for yolo-distance-tuned using images from: /Users/teaguesangster/Code/Python/ComputerVisionFinal/TrainingData/data_object_image_2/training/image_2/\n",
      "Attempting to load model class names from: /Users/teaguesangster/Code/Python/ComputerVisionFinal/ExistingModel/yolo-with-distance/configs/kitty_all_except_nodata.txt\n",
      "Found 6784 label files in /Users/teaguesangster/Code/Python/ComputerVisionFinal/TrainingData/training/label_2/ for . Processing...\n",
      "Warning: Skipped 704 entries because corresponding image files were not found in /Users/teaguesangster/Code/Python/ComputerVisionFinal/TrainingData/data_object_image_2/training/image_2/.\n",
      "Aggregated annotation file created at: /Users/teaguesangster/Code/Python/ComputerVisionFinal/ExistingModel/yolo-with-distance/kitti_train_list_for_keras_yolo_tuned.txt with 6080 image entries.\n",
      "\n",
      "Creating annotation file for yolo-distance-blured using images from: /Users/teaguesangster/Code/Python/ComputerVisionFinal/TrainingData/BlurredImages/BlurredTrainingData/\n",
      "Attempting to load model class names from: /Users/teaguesangster/Code/Python/ComputerVisionFinal/ExistingModel/yolo-with-distance/configs/kitty_all_except_nodata.txt\n",
      "Found 6784 label files in /Users/teaguesangster/Code/Python/ComputerVisionFinal/TrainingData/training/label_2/ for . Processing...\n",
      "Aggregated annotation file created at: /Users/teaguesangster/Code/Python/ComputerVisionFinal/ExistingModel/yolo-with-distance/kitti_train_list_for_keras_yolo_blured.txt with 6784 image entries.\n",
      "--- Cell 3: Annotation File Generation Complete ---\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <font color=\"dodgerblue\">Cell 4: Helper Function - `run_training_session` (Revised)</font>\n",
    "\n",
    "Here we go through a few separate functions that allow me to feel safe leaving the training process alone. This includes including **safety callbacks**, helper functions for **log directory management**, and the main **`run_training_session` function**. <font color=\"chocolate\">The callbacks weren't in the original attempt at training but were added after our loss value sadly exploded.</font>\n",
    "\n",
    "---\n",
    "\n",
    "#### <font color=\"teal\">Safety Callbacks (Keras Callbacks):</font>\n",
    "\n",
    "1.  **`LossGuardCallback`**: <font color=\"darkorange\">To make sure our loss doesn't Explode!</font>\n",
    "    * Here the main thing is we wanted to make sure our total loss values never went over `50`. Confidence could've gone over `50` and did in some sections, but because of its weighting, most of our loss came from distance calculations.\n",
    "    * If total loss went over `50`, an internal flag would be set off (`loss_exploded`).\n",
    "    * At the end of an epoch, if the `loss_exploded` flag is set, it indicates that the checkpoint for that epoch should be considered invalid or skipped. It then resets the flag. <font color=\"chocolate\">This function doesn't *do* the skipping; it just lets us know if we need to skip / not save due to loss.</font>\n",
    "\n",
    "2.  **`EmergencyStopCallback`**: <font color=\"darkorange\">Another Loss managing function!</font>\n",
    "    * Here we are monitoring the training loss at the end of each batch.\n",
    "    * If the loss exceeds a `critical_threshold` (e.g., `1000.0`), it prints a critical warning and signals the Keras model to stop training (`self.model.stop_training = True`). This is a more drastic measure to prevent runaway training with extremely high losses.\n",
    "    * <font color=\"chocolate\">In our initial run, I came back in the morning to see a loss of 2000. At that point, there was no reason to continue training, and as such, that run was effectively removed/restarted.</font>\n",
    "\n",
    "---\n",
    "\n",
    "#### <font color=\"teal\">Re-import `train.py`'s `main` function:</font>\n",
    "* <font color=\"chocolate\">We had issues before with this not being imported correctly, so we call it again just in case.</font> It includes error handling if the import fails.\n",
    "* <font color=\"chocolate\">We have output statements later on as well, as I was making modifications in subdirectories and wanted to make sure messing with those didn't ruin the training run.</font>\n",
    "\n",
    "---\n",
    "\n",
    "#### <font color=\"teal\">Helper Function: `get_latest_timestamped_log_subdir(base_log_dir)`:</font>\n",
    "\n",
    "* **Purpose:** To find the most recently created timestamped subdirectory within a given `base_log_dir`. The `train.py` script is expected to create log directories with names like `\"YYYY_MM_DD_HH_MM_SS\"`.\n",
    "* **Functionality:**\n",
    "    * Checks if `base_log_dir` exists. It then lists all subdirectories in `base_log_dir`. (So we make sure we are handling all our logs correctly).\n",
    "    * We then iterate through these subdirectories, attempting to parse their names using the expected timestamp format.\n",
    "    * We keep track of the subdirectory with the latest modification time whose name matches the timestamp format. This is then returned to be the latest valid timestamped subdirectory.\n",
    "    * <font color=\"chocolate\">Includes a fallback to return the `base_log_dir` itself if no valid timestamped subdirectory is found, with a comment stating this might happen if `train.py` failed early or if the log structure changed.</font>\n",
    "\n",
    "---\n",
    "\n",
    "#### <font color=\"teal\">Helper Function: `find_best_epoch_checkpoint_in_session_log_dir(session_actual_log_dir)`:</font>\n",
    "\n",
    "* **Purpose:** To scan a specific session's actual log directory (which is timestamped and found by the previous function) and identify the best model checkpoint file (e.g., `epXXX-lossYYY-val_lossZZZ.h5`) based on the **lowest validation loss**. <font color=\"chocolate\">We need this as if weights exploded in one run and we still had time leftover, we could re-run and try to improve.</font>\n",
    "* **Functionality:**\n",
    "    * Checks if the provided `session_actual_log_dir` is valid.\n",
    "    * Lists all `.h5` files starting with `\"ep\"` (epoch checkpoints).\n",
    "    * Iterates through these files, parsing the filename to extract the `val_loss` value.\n",
    "    * It includes a safety check to skip checkpoints if their parsed `val_loss` is suspiciously high (e.g., `> 50.0`, matching the `LossGuardCallback` threshold).\n",
    "    * Keeps track of the file with the minimum valid `val_loss`.\n",
    "    * Returns the full path to the best checkpoint file found.\n",
    "\n",
    "---"
   ],
   "id": "15e4d53a0e57074b"
  },
  {
   "cell_type": "code",
   "id": "5657ac4a9422f33b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T01:09:09.252815Z",
     "start_time": "2025-05-14T01:09:08.477825Z"
    }
   },
   "source": [
    "# Cell 4: Helper Function - run_training_session (Revised)\n",
    "# This cell defines several components aimed at managing and executing a single\n",
    "# training session for a YOLO model, including safety callbacks and helper functions\n",
    "# for log directory management.\n",
    "# The callbacks weren't in the original attempt at training but were added\n",
    "# after our loss value sadly exploded.\n",
    "print(\"--- Cell 4: Defining run_training_session function (Revised) ---\")\n",
    "\n",
    "\n",
    "# --- Safety Callbacks (Keras Callbacks) ---\n",
    "# These were added to help manage training and prevent runaway processes,\n",
    "# especially when leaving the model to train unattended.\n",
    "\n",
    "# LossGuardCallback: To make sure our loss doesn't Explode!\n",
    "# Here the main thing is we wanted to make sure our total loss values never went over 50.\n",
    "# Confidence could've gone over 50 and did in some sections, but because of its\n",
    "# weighting most of our loss came from distance calculations.\n",
    "class LossGuardCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold=50.0): # Threshold for \"exploded\" loss.\n",
    "        super(LossGuardCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        self.loss_exploded = False # Flag to indicate if loss has exploded.\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # At the end of each batch, check if the loss exceeds the threshold.\n",
    "        if logs and logs.get('loss') > self.threshold:\n",
    "            print(f\"\\n⚠️ Warning: Loss exploded to {logs.get('loss')}! Marking checkpoint as invalid.\")\n",
    "            self.loss_exploded = True # Set flag if loss is too high.\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # At the end of an epoch, if the loss_exploded flag is set, it indicates that\n",
    "        # the checkpoint for that epoch should be considered invalid or skipped.\n",
    "        # This function doesn't do the skipping it just lets us know if we need to skip / not save due to loss.\n",
    "        if self.loss_exploded:\n",
    "            print(f\"⚠️ Epoch {epoch+1} checkpoint will be skipped due to loss explosion.\")\n",
    "            self.loss_exploded = False  # Reset for next epoch to monitor anew.\n",
    "\n",
    "# EmergencyStopCallback: Another Loss managing function!\n",
    "# Here we are monitoring the training loss at the end of each batch.\n",
    "# If the loss exceeds a critical_threshold, it stops training.\n",
    "# In our initial run I came back in the morning to see a loss of 2000. At that point\n",
    "# there was no reason to continue training and as such it was removed (that run).\n",
    "class EmergencyStopCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, critical_threshold=1000.0): # Higher threshold for emergency stop.\n",
    "        super(EmergencyStopCallback, self).__init__()\n",
    "        self.critical_threshold = critical_threshold\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # If loss exceeds the critical threshold, stop the training.\n",
    "        if logs and logs.get('loss') > self.critical_threshold:\n",
    "            print(f\"\\n🛑 CRITICAL: Loss exploded to {logs.get('loss')}! Stopping training.\")\n",
    "            self.model.stop_training = True # Signal Keras model to stop.\n",
    "\n",
    "\n",
    "# --- Re-import train.py's main function ---\n",
    "# We had issues before with this not being imported correctly so we call it again just in case.\n",
    "# We have output statements later on as well as I was making modification in subdirectories\n",
    "# and wanted to make sure messing with those didn't ruin the training run.\n",
    "try:\n",
    "    from train import main as run_yolo_training_script # Imported from train.py from the yolo-with-distance repo.\n",
    "    print(\"Successfully re-imported 'main' as 'run_yolo_training_script' from train.py for this cell.\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: Could not import 'main' from train.py in Cell 4. Ensure train.py is accessible.\")\n",
    "    run_yolo_training_script = None # Set to None if import fails to prevent errors later.\n",
    "\n",
    "\n",
    "# --- Helper Function: get_latest_timestamped_log_subdir ---\n",
    "def get_latest_timestamped_log_subdir(base_log_dir):\n",
    "    \"\"\"\n",
    "    Finds the most recent timestamped subdirectory within the base_log_dir.\n",
    "    The train.py script is expected to create log directories with names like \"YYYY_MM_DD_HH_MM_SS\".\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(base_log_dir): # Ensure base_log_dir itself exists.\n",
    "        print(f\"Warning: Base log directory {base_log_dir} does not exist.\")\n",
    "        return None\n",
    "        \n",
    "    subdirs = [] # List to store paths of subdirectories.\n",
    "    # Lists all subdirectories in base_log_dir. (So we make sure we are handling all our logs correctly).\n",
    "    for d_name in os.listdir(base_log_dir):\n",
    "        d_path = os.path.join(base_log_dir, d_name)\n",
    "        if os.path.isdir(d_path): # Check if it's a directory.\n",
    "            subdirs.append(d_path)\n",
    "            \n",
    "    if not subdirs:\n",
    "        print(f\"No subdirectories found in {base_log_dir}.\")\n",
    "        return None\n",
    "    \n",
    "    latest_subdir_found = None # To store the path of the latest valid subdirectory.\n",
    "    latest_time = 0 # To store the modification time of the latest valid subdirectory.\n",
    "\n",
    "    # We then Iterate through these subdirectories, attempting to parse their names\n",
    "    # using the expected timestamp format.\n",
    "    for subdir_path in subdirs:\n",
    "        try:\n",
    "            # Check if dirname matches expected timestamp format from train.py\n",
    "            datetime.strptime(os.path.basename(subdir_path), \"%Y_%m_%d_%H_%M_%S\")\n",
    "            # If format matches, check its modification time.\n",
    "            mod_time = os.path.getmtime(subdir_path)\n",
    "            # We keep track of the subdirectory with the latest modification time\n",
    "            # whose name matches the timestamp format.\n",
    "            if mod_time > latest_time:\n",
    "                latest_time = mod_time\n",
    "                latest_subdir_found = subdir_path\n",
    "        except ValueError:\n",
    "            # Subdirectory name doesn't match the timestamp format, ignore it.\n",
    "            continue\n",
    "            \n",
    "    if latest_subdir_found:\n",
    "        print(f\"Identified latest timestamped log directory: {latest_subdir_found}\")\n",
    "    else:\n",
    "        print(f\"Could not find a valid timestamped log directory in {base_log_dir}. Will attempt to scan base dir.\")\n",
    "        # Fallback to return the base_log_dir itself if no valid timestamped subdirectory is found,\n",
    "        # This might happen if train.py failed early or if the log structure changed.\n",
    "        return base_log_dir # Or None, depending on how strictly we want to enforce timestamped dirs.\n",
    "        \n",
    "    return latest_subdir_found # This is then returned to be the latest valid timestamped subdirectory.\n",
    "\n",
    "\n",
    "# --- Helper Function: find_best_epoch_checkpoint_in_session_log_dir ---\n",
    "def find_best_epoch_checkpoint_in_session_log_dir(session_actual_log_dir):\n",
    "    \"\"\"\n",
    "    Scans a specific session's actual log directory (timestamped) for epoch checkpoint files\n",
    "    (e.g., epXXX-lossYYY-val_lossZZZ.h5) and returns path to the one with lowest val_loss.\n",
    "    We need this as if weights exploded in one run and we still had time leftover, we could re-run and try to improve.\n",
    "    \"\"\"\n",
    "    # Checks if the provided session_actual_log_dir is valid.\n",
    "    if not session_actual_log_dir or not os.path.isdir(session_actual_log_dir):\n",
    "        print(f\"Error: Session log directory '{session_actual_log_dir}' is invalid or not provided.\")\n",
    "        return None\n",
    "\n",
    "    # Lists all .h5 files starting with \"ep\" (epoch checkpoints).\n",
    "    checkpoint_files = [f for f in os.listdir(session_actual_log_dir) if f.startswith('ep') and f.endswith('.h5')]\n",
    "    best_val_loss = float('inf') # Initialize with a very high value.\n",
    "    best_checkpoint_file_path = None # To store the path of the best checkpoint.\n",
    "\n",
    "    if not checkpoint_files:\n",
    "        print(f\"No epoch checkpoint files (ep*.h5) found in {session_actual_log_dir}.\")\n",
    "        return None\n",
    "\n",
    "    # Iterates through these files, parsing the filename to extract the val_loss value.\n",
    "    for fname in checkpoint_files:\n",
    "        try:\n",
    "            parts = fname.replace('.h5', \"\").split('-') # Split filename by '-' to find val_loss part.\n",
    "            val_loss_str = None\n",
    "            for part in parts:\n",
    "                if part.startswith('val_loss'): # Look for the part starting with 'val_loss'.\n",
    "                    val_loss_str = part.replace('val_loss', '') # Extract the numeric value.\n",
    "                    break\n",
    "            if val_loss_str:\n",
    "                val_loss = float(val_loss_str)\n",
    "                \n",
    "                # It includes a safety check to skip checkpoints if their parsed val_loss is\n",
    "                # suspiciously high (e.g., > 50.0, matching the LossGuardCallback threshold).\n",
    "                if val_loss > 50.0:  # Same threshold as in LossGuardCallback.\n",
    "                    print(f\"⚠️ Skipping checkpoint {fname} with suspiciously high loss {val_loss}\")\n",
    "                    continue # Skip this checkpoint.\n",
    "                \n",
    "                # Keeps track of the file with the minimum valid val_loss.\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_checkpoint_file_path = os.path.join(session_actual_log_dir, fname)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing val_loss from {fname} in {session_actual_log_dir}: {e}\")\n",
    "\n",
    "    if best_checkpoint_file_path:\n",
    "        print(f\"Identified best session checkpoint: {os.path.basename(best_checkpoint_file_path)} with val_loss: {best_val_loss:.3f}\")\n",
    "    else:\n",
    "        print(f\"Could not determine a best session checkpoint file in {session_actual_log_dir} from epXXX.h5 files.\")\n",
    "            \n",
    "    return best_checkpoint_file_path # Returns the full path to the best checkpoint file found.\n",
    "# No main run_training_session function in this cell according to the provided code block."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 4: Defining run_training_session function (Revised) ---\n",
      "Successfully re-imported 'main' as 'run_yolo_training_script' from train.py for this cell.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### <font color=\"darkorange\">Now for the Main Function!!! `run_training_session(...)`:</font>\n",
    "Because we are running two models with different training parameters, we have a main function that can take in which model it needs (`model_name_str` for \"tuned\" or \"blurred\"). We also pass if we need it for a given number of epochs. It handles loading weights, setting up training arguments, calling the external training script, and then saving the best checkpoint from that session.\n",
    "\n",
    "* **Parameters:**\n",
    "    * `model_name_str`: What model we actually want to train! (e.g., `\"tuned\"`, `\"blurred\"`).\n",
    "    * `current_total_fine_tune_epochs_for_model`: The total number of epochs this model has already been fine-tuned for in previous sessions (used to calculate `init_epoch`).\n",
    "    * `annotation_file_path`: Path to the aggregated annotation file for the dataset.\n",
    "    * `model_base_log_dir`: The base directory where logs for this model type are stored (e.g., `log_dir_base_tuned`).\n",
    "    * `target_overall_best_h5_path`: The path where the overall best checkpoint for this model type should be saved (e.g., `tuned_model_best_h5_path`).\n",
    "    * `epochs_to_run_this_session`: The number of epochs to train for in this specific call.\n",
    "* **Functionality:**\n",
    "    * **Weight Loading:** Determines which weights to load. If a `target_overall_best_h5_path` exists, it loads those weights; otherwise, it falls back to `initial_weights_path`.\n",
    "    * **Directory Creation:** Ensures `model_base_log_dir` exists.\n",
    "    * **Epoch Calculation:** Calculates `session_init_epoch` (starting epoch number for `train.py`) and `session_total_epoch` (target ending epoch number).\n",
    "    * **Argument Setup (`argparse.Namespace`):** Creates an `argparse.Namespace` object (`train_args_for_session`) to simulate command-line arguments for `train.py`. This is where all training hyperparameters are set:\n",
    "        * Uses `common_yolo_params` (from Cell 1) for model type, anchor/class paths, image size.\n",
    "        * Sets `weights_path` to the determined weights to load.\n",
    "        * Sets `annotation_file`, `log_dir`.\n",
    "        * Specifies `batch_size` (modified to `8`), `learning_rate` (very low, `1e-7`), `optimizer` (modified to `'rmsprop'` for ARM compatibility), `clipnorm`, `clipvalue` (added for stability). <font color=\"chocolate\">You can feel free to change this, but again we were limited by training hardware.</font>\n",
    "        * Sets `freeze_level` (modified to `2`, freeze more layers), `label_smoothing` (added).\n",
    "        * `elim_grid_sense` is set to `True` (changed to always `True` to prevent confidence loss explosion).\n",
    "        * Sets `workers` and `max_queue_size` (modified/reduced for ARM).\n",
    "        * Sets `loss_weights` with custom values to balance confidence, location, class, and distance components.\n",
    "        * **<font color=\"red\">Important Modification:</font>** `eval_online` is set to `False` to disable online evaluation during training. I couldn't get this working but maybe you can! \n",
    "    * **GPU Configuration:** Clears the Keras backend session and attempts to set memory growth for physical GPUs to `True` to avoid TensorFlow allocating all GPU memory at once.\n",
    "    * **Call Training Script:** Calls `run_yolo_training_script(train_args_for_session)`, which is the imported `main` function from `train.py`.\n",
    "    * **Post-Training Checkpoint Handling:**\n",
    "        * After the training script completes, it calls `get_latest_timestamped_log_subdir` to find the actual log directory created by `train.py` for this session.\n",
    "        * Then, it calls `find_best_epoch_checkpoint_in_session_log_dir` to find the best `.h5` checkpoint within that session's log directory.\n",
    "        * **Loss Check on Filename:** It performs a basic string check on the filename of the best checkpoint for indicators of very high loss (e.g., `\"loss1000\"`). If such indicators are found, it prints a warning and does *not* update the `target_overall_best_h5_path`. Again we only want to save the best run!\n",
    "        * If a valid best checkpoint is found (and no high loss indicators in its name), it copies this checkpoint to `target_overall_best_h5_path` using `shutil.copy2`.\n",
    "        * Includes warnings if no best checkpoint is found or if the specific log directory couldn't be identified.\n",
    "    * **Error Handling:** Wraps the training script call and checkpoint handling in a `try-except` block to catch any exceptions during training, print a traceback, and set `session_success` to `False`.\n",
    "    * **Return Values:** Returns `session_success` (boolean) and `epochs_actually_run` (integer).\n",
    "\n",
    "*The cell concludes with print statements indicating its definition and noting the \"Revised with eval_online=False\" status. The key purpose of this cell is to provide a robust wrapper around the external `train.py` script, allowing for iterative training sessions, management of weights and logs, and safety checks against exploding losses.*"
   ],
   "id": "3c6871b30a893e9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T01:09:09.261117Z",
     "start_time": "2025-05-14T01:09:09.253507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modified run_training_session function\n",
    "# --- Main Function: run_training_session(...) ---\n",
    "# Because we are running two models with different training parameters we have a main\n",
    "# function that can take in which model it needs (model_name_str for \"tuned\" or \"blurred\").\n",
    "# We also pass if we need it for a given number of epochs. It handles loading weights,\n",
    "# setting up training arguments, calling the external training script, and then\n",
    "# saving the best checkpoint from that session.\n",
    "def run_training_session(model_name_str, # What model we actually want to train! (e.g., \"tuned\", \"blurred\").\n",
    "                         current_total_fine_tune_epochs_for_model, # The total number of epochs this model has already been fine-tuned for in previous sessions (used to calculate init_epoch).\n",
    "                         annotation_file_path, # Path to the aggregated annotation file for the dataset.\n",
    "                         model_base_log_dir, # The base directory where logs for this model type are stored (e.g., log_dir_base_tuned).\n",
    "                         target_overall_best_h5_path, # The path where the overall best checkpoint for this model type should be saved (e.g., tuned_model_best_h5_path).\n",
    "                         epochs_to_run_this_session): # The number of epochs to train for in this specific call.\n",
    "    if run_yolo_training_script is None: # Check if the training script function was imported successfully.\n",
    "        print(\"CRITICAL ERROR: run_yolo_training_script (from train.py) is not defined. Cannot train.\")\n",
    "        return False, 0 # Return failure and 0 epochs run.\n",
    "\n",
    "    print(f\"\\n--- Preparing Training Session for: {model_name_str} ---\")\n",
    "    print(f\"Will attempt to train for {epochs_to_run_this_session} epochs this session.\")\n",
    "    \n",
    "    # Weight Loading: Determines which weights to load.\n",
    "    weights_to_load = initial_weights_path # Default to initial weights.\n",
    "    # If an target_overall_best_h5_path exists, it loads those weights;\n",
    "    if os.path.exists(target_overall_best_h5_path):\n",
    "        weights_to_load = target_overall_best_h5_path # Use previously saved best weights.\n",
    "        print(f\"Loading previously saved best weights for {model_name_str} from: {weights_to_load}\")\n",
    "    else:\n",
    "        # otherwise, it falls back to initial_weights_path.\n",
    "        print(f\"No previous overall best weights found at {target_overall_best_h5_path}. Loading initial weights: {initial_weights_path}\")\n",
    "\n",
    "    # Directory Creation: Ensures model_base_log_dir exists.\n",
    "    os.makedirs(model_base_log_dir, exist_ok=True)\n",
    "    \n",
    "    # Epoch Calculation: Calculates session_init_epoch (starting epoch number for train.py)\n",
    "    # and session_total_epoch (target ending epoch number).\n",
    "    session_init_epoch = BASE_EPOCHS_ORIGINAL_MODEL + current_total_fine_tune_epochs_for_model\n",
    "    session_total_epoch = session_init_epoch + epochs_to_run_this_session\n",
    "\n",
    "    # Argument Setup (argparse.Namespace): Creates an argparse.Namespace object\n",
    "    # (train_args_for_session) to simulate command-line arguments for train.py.\n",
    "    # This is where all training hyperparameters are set:\n",
    "    train_args_for_session = argparse.Namespace(\n",
    "        # Uses common_yolo_params (from Cell 1) for model type, anchor/class paths, image size.\n",
    "        model_type=common_yolo_params['model_type'],\n",
    "        weights_path=weights_to_load, # Sets weights_path to the determined weights to load.\n",
    "        annotation_file=annotation_file_path, # Sets annotation_file, log_dir.\n",
    "        anchors_path=common_yolo_params['anchors_path'],\n",
    "        classes_path=common_yolo_params['classes_path'],\n",
    "        model_image_size=common_yolo_params['model_image_size'],\n",
    "        # Specifies batch_size (modified to 8), learning_rate (very low, 1e-7),\n",
    "        # optimizer (modified to 'rmsprop' for ARM compatibility), clipnorm,\n",
    "        # clipvalue (added for stability). You can feel free to change this but\n",
    "        # again we were limited by training hardware.\n",
    "        batch_size=8,  # MODIFIED: Reduced batch size for stability\n",
    "        init_epoch=session_init_epoch,\n",
    "        total_epoch=session_total_epoch,\n",
    "        learning_rate=1e-7,  # Keep this very low\n",
    "        optimizer='rmsprop',  # MODIFIED: Switch to rmsprop for ARM compatibility\n",
    "        clipnorm=5.0,  # MODIFIED: Reasonable gradient clipping\n",
    "        clipvalue=10.0, # ADDED: Also clip by value for additional stability\n",
    "        log_dir=model_base_log_dir, \n",
    "        checkpoint_period=1, \n",
    "        val_split=0.1,\n",
    "        val_annotation_file=None,\n",
    "        freeze_level=2,  # MODIFIED: Freeze more layers to stabilize training (Sets freeze_level (modified to 2, freeze more layers), label_smoothing (added)).\n",
    "        transfer_epoch=0,\n",
    "        multiscale=False,\n",
    "        rescale_interval=10,\n",
    "        enhance_augment=None,\n",
    "        label_smoothing=0.1,  # MODIFIED: Added some label smoothing\n",
    "        multi_anchor_assign=False,\n",
    "        elim_grid_sense=True,  # Changed to always True to prevent confidence loss explosion.\n",
    "        data_shuffle=True,\n",
    "        gpu_num=1,\n",
    "        model_pruning=False,\n",
    "        eval_online=False, # Important Modification: eval_online is set to False to disable online evaluation during training, likely to avoid potential errors or complexities with the EvalCallBack.\n",
    "        eval_epoch_interval=1, \n",
    "        save_best_only=True, \n",
    "        save_eval_checkpoint=False,\n",
    "        dataset_working_directory=\"\", \n",
    "        decay_type=None,\n",
    "        lr_patience=5,\n",
    "        min_lr=1e-8,\n",
    "        early_stopping_patience=10,\n",
    "        workers=8,  # MODIFIED: Reduced workers for ARM processors (Sets workers and max_queue_size (modified/reduced for ARM)).\n",
    "        use_multiprocessing=True, \n",
    "        max_queue_size=32,  # MODIFIED: Reduced queue size\n",
    "        # Updated loss weights to better balance components (Sets loss_weights with custom values to balance confidence, location, class, and distance components).\n",
    "        loss_weights={'confidence': 0.01, 'location': 1.0, 'class': 1.0, 'dist': 3.0},\n",
    "    )\n",
    "    if not train_args_for_session.eval_online:\n",
    "        print(\"NOTE: Online evaluation (EvalCallBack) is disabled for this session to avoid potential errors.\")\n",
    "    \n",
    "    print(f\"Effective training epochs for {model_name_str}: {train_args_for_session.init_epoch} to {train_args_for_session.total_epoch -1}.\")\n",
    "\n",
    "    session_success = False # Flag to track if the session completed without critical errors.\n",
    "    epochs_actually_run = 0 # Counter for epochs run in this session.\n",
    "    \n",
    "    # Error Handling: Wraps the training script call and checkpoint handling in a\n",
    "    # try-except block to catch any exceptions during training, print a traceback,\n",
    "    # and set session_success to False.\n",
    "    try:\n",
    "        # GPU Configuration: Clears the Keras backend session and attempts to set memory\n",
    "        # growth for physical GPUs to True to avoid TensorFlow allocating all GPU memory at once.\n",
    "        print(f\"Clearing Keras session and configuring GPU for {model_name_str} training...\")\n",
    "        tf.keras.backend.clear_session() # Clear previous Keras states.\n",
    "        physical_gpus = tf.config.list_physical_devices('GPU') # Get list of GPUs.\n",
    "        if physical_gpus:\n",
    "            try:\n",
    "                # Attempt to enable memory growth for each GPU.\n",
    "                for gpu in physical_gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            except RuntimeError as e:\n",
    "                # This error can occur if memory growth is already set or cannot be set.\n",
    "                print(f\"Note: GPU memory growth setting issue (may be already set): {e}\")\n",
    "        else:\n",
    "            print(f\"WARNING: No GPU detected by TensorFlow for {model_name_str} session!\")\n",
    "\n",
    "        # Call Training Script: Calls run_yolo_training_script(train_args_for_session),\n",
    "        # which is the imported main function from train.py.\n",
    "        print(f\"Calling training script for {model_name_str}...\")\n",
    "        run_yolo_training_script(train_args_for_session) # Execute the training.\n",
    "        print(f\"Training script call completed for {model_name_str}.\")\n",
    "        \n",
    "        epochs_actually_run = epochs_to_run_this_session # Assume all requested epochs ran if no exception.\n",
    "        session_success = True # Mark session as successful.\n",
    "\n",
    "        # Post-Training Checkpoint Handling:\n",
    "        # Since train.py creates a timestamped subdirectory, we need to find it.\n",
    "        # After the training script completes, it calls get_latest_timestamped_log_subdir\n",
    "        # to find the actual log directory created by train.py for this session.\n",
    "        session_specific_log_dir_actual = get_latest_timestamped_log_subdir(model_base_log_dir)\n",
    "        \n",
    "        if session_specific_log_dir_actual:\n",
    "            print(f\"Scanning for best checkpoint in actual session log directory: {session_specific_log_dir_actual}\")\n",
    "            # Then, it calls find_best_epoch_checkpoint_in_session_log_dir to find the\n",
    "            # best .h5 checkpoint within that session's log directory.\n",
    "            best_checkpoint_from_this_session = find_best_epoch_checkpoint_in_session_log_dir(session_specific_log_dir_actual)\n",
    "            \n",
    "            if best_checkpoint_from_this_session and os.path.exists(best_checkpoint_from_this_session):\n",
    "                # Loss Check on Filename: It performs a basic string check on the filename\n",
    "                # of the best checkpoint for indicators of very high loss (e.g., \"loss1000\").\n",
    "                checkpoint_filename = os.path.basename(best_checkpoint_from_this_session)\n",
    "                \n",
    "                # Simple string check for suspiciously high loss values in filename.\n",
    "                high_loss_indicators = [\"loss100\", \"loss200\", \"loss500\", \"loss1000\"]\n",
    "                # If such indicators are found, it prints a warning and does *not*\n",
    "                # update the target_overall_best_h5_path.\n",
    "                if any(indicator in checkpoint_filename for indicator in high_loss_indicators):\n",
    "                    print(f\"⚠️ WARNING: Best checkpoint filename indicates very high loss: {checkpoint_filename}\")\n",
    "                    print(f\"The overall best model at {target_overall_best_h5_path} will NOT be updated.\")\n",
    "                else:\n",
    "                    # If a valid best checkpoint is found (and no high loss indicators in its name),\n",
    "                    # it copies this checkpoint to target_overall_best_h5_path using shutil.copy2.\n",
    "                    print(f\"Best checkpoint from this session found: {best_checkpoint_from_this_session}\")\n",
    "                    print(f\"Copying to target overall best model path: {target_overall_best_h5_path}\")\n",
    "                    shutil.copy2(best_checkpoint_from_this_session, target_overall_best_h5_path) # Copy the best session checkpoint.\n",
    "                    print(f\"Successfully updated overall best model for {model_name_str} at: {target_overall_best_h5_path}\")\n",
    "            else:\n",
    "                # Includes warnings if no best checkpoint is found or if the specific log\n",
    "                # directory couldn't be identified.\n",
    "                print(f\"WARNING: No best checkpoint found in {session_specific_log_dir_actual} from this session for {model_name_str}.\")\n",
    "                print(f\"The overall best model at {target_overall_best_h5_path} was NOT updated this session.\")\n",
    "        else:\n",
    "            print(f\"WARNING: Could not identify the specific log subdirectory for this session in {model_base_log_dir}. Cannot copy best model.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! ERROR during training script execution for {model_name_str}: {e} !!!\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for debugging.\n",
    "        session_success = False # Mark session as failed.\n",
    "        epochs_actually_run = 0 # No epochs considered run if there was an error.\n",
    "\n",
    "    print(f\"--- Training Session for {model_name_str} Concluded (Success: {session_success}, Epochs run this session: {epochs_actually_run}) ---\")\n",
    "    # Return Values: Returns session_success (boolean) and epochs_actually_run (integer).\n",
    "    return session_success, epochs_actually_run\n",
    "# The cell concludes with print statements indicating its definition and noting\n",
    "# the \"Revised with eval_online=False\" status. The key purpose of this cell is\n",
    "# to provide a robust wrapper around the external train.py script, allowing for\n",
    "# iterative training sessions, management of weights and logs, and safety checks\n",
    "# against exploding losses.\n",
    "print(\"--- Cell 4: Definition Complete (Revised with eval_online=False) ---\")"
   ],
   "id": "89c3082e29a60313",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 4: Definition Complete (Revised with eval_online=False) ---\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "8b649af851e73e7c",
   "metadata": {},
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Test Run Configuration:\n",
    "### <font color=\"dodgerblue\">Here is our trial run!</font>\n",
    "\n",
    "Because we have so much happening in both of our models, and because our main loop was designed to go for so long, it became clear a testing cell was required. The most important part of this is ensuring the pipeline for both the \"tuned\" (normal images) and \"blurred\" (blurred images) models. We do this by ensuring that the run_training_session function (defined in Cell 4) can execute for a minimal number of epochs without critical errors, using dedicated test log directories and test model save paths.\n",
    "\n",
    "Here's the breakdown:\n",
    "\n",
    "1.  **<font color=\"teal\">Test Run Configuration:</font>**\n",
    "    * `EPOCHS_FOR_TEST_RUN` is set to `1`. This means each model will only attempt to train for a single epoch during this test phase, making it quick.\n",
    "    * The cell prints messages to clearly indicate the start of this test phase and the (short) number of epochs to be used.\n",
    "\n",
    "2.  **<font color=\"teal\">Test `yolo-distance-tuned` (Normal Images Model):</font>**\n",
    "    * A specific path for where the \"best\" model from *this particular test run* would be saved is defined (`test_best_h5_path_tuned`). This path is within the `tuned_model_save_dir` (from Cell 1) but has a `_QUICK_TEST_best.h5` suffix to distinguish it from actual training run outputs.\n",
    "    * Informative messages are printed, noting the start of the test for the \"tuned\" model, where its test logs will be stored (in `test_log_dir_base_tuned`, also from Cell 1), and the target save path for its test model.\n",
    "    * The `run_training_session` function is then called with the following key parameters:\n",
    "        * `model_name_str`: `\"yolo-distance-tuned (Test)\"`\n",
    "        * `current_total_fine_tune_epochs_for_model`: `0` <font color=\"chocolate\">(since it's a fresh test, we're not continuing previous fine-tuning for this test sequence; it will usually just load the `initial_weights_path`).</font>\n",
    "        * `annotation_file_path`: `annotation_file_tuned` <font color=\"chocolate\">(This variable, pointing to the aggregated annotation file for normal images generated in Cell 2, is have been defined in a preceding cell.</font>\n",
    "        * `model_base_log_dir`: `test_log_dir_base_tuned`\n",
    "        * `target_overall_best_h5_path`: `test_best_h5_path_tuned`\n",
    "        * `epochs_to_run_this_session`: `EPOCHS_FOR_TEST_RUN` (which is 1)\n",
    "    * After the `run_training_session` call, the script checks the `success_test_tuned` flag and the `epochs_done_test_tuned` value.\n",
    "    * If the session was successful and the epoch completed, a success message is printed. It then also checks if the `test_best_h5_path_tuned` file was actually created (this might not happen if, for example, the validation loss didn't improve in the single epoch, and `save_best_only` is True in `train.py`).\n",
    "    * If the session was not successful or didn't complete the epoch, a failure message is printed, advising the user to check the logs for errors.\n",
    "\n",
    "3.  **<font color=\"teal\">Break Time:</font>**\n",
    "    * A `time.sleep(BREAK_TIME_SECONDS)` is called (using the `BREAK_TIME_SECONDS` constant defined in Cell 1). This introduces a pause before the script proceeds to test the next model. <font color=\"chocolate\">We do this so that I don't get too much memorry pressure on my MAC, but if you were running this in collab I doubt you would need this extra time to avoid a crash or performance issues.</font>\n",
    "\n",
    "4.  **<font color=\"teal\">Test `yolo-distance-blured` (Blurred Images Model):</font>**\n",
    "    * This section mirrors the test for the \"tuned\" model but targets the model configuration for blurred images.\n",
    "    * A specific path for the \"best\" model from *this test run* is defined (`test_best_h5_path_blurred`), again using a `_QUICK_TEST_best.h5` suffix.\n",
    "    * Messages are printed indicating the start of this test, its log directory (`test_log_dir_base_blurred` from Cell 1), and its model save path. There's alot of comments here but that's because you never know what breaks and comments tell us where we were at.\n",
    "    * `run_training_session` is called with parameters tailored for the blurred model:\n",
    "        * `model_name_str`: `\"yolo-distance-blured (Test)\"`\n",
    "        * `current_total_fine_tune_epochs_for_model`: `0`\n",
    "        * `annotation_file_path`: `annotation_file_blurred` <font color=\"chocolate\">(Similar to `annotation_file_tuned`, this path to the annotation file for the blurred image dataset, generated in Cell 2, must have been defined in a preceding cell, likely Cell 3).</font>\n",
    "        * `model_base_log_dir`: `test_log_dir_base_blurred`\n",
    "        * `target_overall_best_h5_path`: `test_best_h5_path_blurred`\n",
    "        * `epochs_to_run_this_session`: `EPOCHS_FOR_TEST_RUN` (which is 1)\n",
    "    * The success and epoch completion for this blurred model test run are then checked, with corresponding success/failure messages and verification for the saved test model file.\n",
    "\n",
    "5.  **<font color=\"teal\">Conclusion and Review Instructions:</font>**\n",
    "    * The cell finally will print a message indicating that the \"Quick Test Run Phase\" is complete.\n",
    "    * **<font color=\"red\">Crucially</font>**, Finally some important warnings for my groupmates!\n",
    "        * `>>> IMPORTANT: Review the output and logs from this test phase carefully (check the _test log directories). <<<`\n",
    "        * `>>> Verify that 'Successfully updated best model...' messages appeared if expected, or that checkpoints were created. <<<`\n",
    "       "
   ],
   "id": "3efc1e57ebc6d07e"
  },
  {
   "cell_type": "code",
   "id": "6c2c195d05fbf55a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-14T01:09:09.261785Z"
    }
   },
   "source": [
    "# Cell 5: Quick Test Run\n",
    "# Here is our trial run!\n",
    "# Because we have so much happening in both of our models, and because our main\n",
    "# loop was designed to go for so long, it became clear a testing cell was required.\n",
    "# The most important part of this is ensuring the pipeline for both the \"tuned\"\n",
    "# (normal images) and \"blurred\" (blurred images) models. We do this by ensuring\n",
    "# that the run_training_session function (defined in Cell 4) can execute for a\n",
    "# minimal number of epochs without critical errors, using dedicated test log\n",
    "# directories and test model save paths.\n",
    "print(\"--- Cell 5: Quick Test Run Phase ---\")\n",
    "\n",
    "# 1. Test Run Configuration:\n",
    "EPOCHS_FOR_TEST_RUN = 1 # This means each model will only attempt to train for a single epoch during this test phase, making it quick.\n",
    "print(f\"Quick test run will use {EPOCHS_FOR_TEST_RUN} epoch(s) per model.\")\n",
    "\n",
    "# --- Test yolo-distance-tuned (Normal Images Model) ---\n",
    "# 2. Test `yolo-distance-tuned` (Normal Images Model):\n",
    "# A specific path for where the \"best\" model from *this particular test run*\n",
    "# would be saved is defined (`test_best_h5_path_tuned`). This path is within the\n",
    "# `tuned_model_save_dir` (from Cell 1) but has a `_QUICK_TEST_best.h5` suffix\n",
    "# to distinguish it from actual training run outputs.\n",
    "test_best_h5_path_tuned = os.path.join(tuned_model_save_dir, \"yolo-distance-tuned_QUICK_TEST_best.h5\")\n",
    "# Informative messages are printed, noting the start of the test for the \"tuned\" model,\n",
    "# where its test logs will be stored (in `test_log_dir_base_tuned`, also from Cell 1),\n",
    "# and the target save path for its test model.\n",
    "print(f\"\\n--- Starting QUICK TEST RUN for yolo-distance-tuned ---\")\n",
    "print(f\"Test logs will go into subdirectories of: {test_log_dir_base_tuned}\")\n",
    "print(f\"Test best model will attempt to save to: {test_best_h5_path_tuned}\")\n",
    "\n",
    "# The `run_training_session` function is then called with the following key parameters:\n",
    "# For a test run, current_total_fine_tune_epochs_for_model is 0 (since it's a fresh test,\n",
    "# we're not continuing previous fine-tuning for this test sequence; it will usually just load\n",
    "# the `initial_weights_path`).\n",
    "# `annotation_file_path`: `annotation_file_tuned` (This variable, pointing to the\n",
    "# aggregated annotation file for normal images generated in Cell 2, must have been defined\n",
    "# in a preceding cell).\n",
    "success_test_tuned, epochs_done_test_tuned = run_training_session(\n",
    "    model_name_str=\"yolo-distance-tuned (Test)\",\n",
    "    current_total_fine_tune_epochs_for_model=0, \n",
    "    annotation_file_path=annotation_file_tuned, # Assumes annotation_file_tuned is defined previously\n",
    "    model_base_log_dir=test_log_dir_base_tuned, \n",
    "    target_overall_best_h5_path=test_best_h5_path_tuned, \n",
    "    epochs_to_run_this_session=EPOCHS_FOR_TEST_RUN\n",
    ")\n",
    "\n",
    "# After the `run_training_session` call, the script checks the `success_test_tuned` flag\n",
    "# and the `epochs_done_test_tuned` value.\n",
    "if success_test_tuned and epochs_done_test_tuned == EPOCHS_FOR_TEST_RUN:\n",
    "    print(\"SUCCESS: Quick test run for yolo-distance-tuned appears to have completed the epoch(s).\")\n",
    "    # It then also checks if the `test_best_h5_path_tuned` file was actually created\n",
    "    # (this might not happen if, for example, the validation loss didn't improve in\n",
    "    # the single epoch, and `save_best_only` is True in `train.py`).\n",
    "    if os.path.exists(test_best_h5_path_tuned):\n",
    "        print(f\"  Test best model saved at {test_best_h5_path_tuned}\")\n",
    "    else:\n",
    "        print(f\"  Warning: Test best model for tuned was NOT found at {test_best_h5_path_tuned} (this might be okay if val_loss didn't improve in 1 epoch).\")\n",
    "else:\n",
    "    # If the session was not successful or didn't complete the epoch, a failure message\n",
    "    # is printed, advising the user to check the logs for errors.\n",
    "    print(\"FAILED: Quick test run for yolo-distance-tuned encountered issues or did not complete epochs. Please check logs above.\")\n",
    "\n",
    "# 3. Break Time:\n",
    "# A `time.sleep(BREAK_TIME_SECONDS)` is called (using the `BREAK_TIME_SECONDS`\n",
    "# constant defined in Cell 1). This introduces a pause before the script proceeds\n",
    "# to test the next model. We do this so that I don't get too much memory pressure on my MAC,\n",
    "# but if you were running this in collab I doubt you would need this extra time\n",
    "# to avoid a crash or performance issues.\n",
    "print(f\"\\nTaking a {BREAK_TIME_SECONDS} second break before testing the blurred model...\")\n",
    "time.sleep(BREAK_TIME_SECONDS)\n",
    "\n",
    "# --- Test yolo-distance-blured (Blurred Images Model) ---\n",
    "# 4. Test `yolo-distance-blured` (Blurred Images Model):\n",
    "# This section mirrors the test for the \"tuned\" model but targets the model\n",
    "# configuration for blurred images.\n",
    "# A specific path for the \"best\" model from *this test run* is defined\n",
    "# (`test_best_h5_path_blurred`), again using a `_QUICK_TEST_best.h5` suffix.\n",
    "test_best_h5_path_blurred = os.path.join(blurred_model_save_dir, \"yolo-distance-blured_QUICK_TEST_best.h5\")\n",
    "# Messages are printed indicating the start of this test, its log directory\n",
    "# (`test_log_dir_base_blurred` from Cell 1), and its model save path.\n",
    "# There's a lot of comments here but that's because you never know what breaks\n",
    "# and comments tell us where we were at.\n",
    "print(f\"\\n--- Starting QUICK TEST RUN for yolo-distance-blured ---\")\n",
    "print(f\"Test logs will go into subdirectories of: {test_log_dir_base_blurred}\")\n",
    "print(f\"Test best model will attempt to save to: {test_best_h5_path_blurred}\")\n",
    "\n",
    "# `run_training_session` is called with parameters tailored for the blurred model:\n",
    "# `annotation_file_path`: `annotation_file_blurred` (Similar to `annotation_file_tuned`,\n",
    "# this path to the annotation file for the blurred image dataset, generated in Cell 2,\n",
    "# must have been defined in a preceding cell).\n",
    "success_test_blurred, epochs_done_test_blurred = run_training_session(\n",
    "    model_name_str=\"yolo-distance-blured (Test)\",\n",
    "    current_total_fine_tune_epochs_for_model=0,\n",
    "    annotation_file_path=annotation_file_blurred, # Assumes annotation_file_blurred is defined previously\n",
    "    model_base_log_dir=test_log_dir_base_blurred, \n",
    "    target_overall_best_h5_path=test_best_h5_path_blurred,\n",
    "    epochs_to_run_this_session=EPOCHS_FOR_TEST_RUN\n",
    ")\n",
    "\n",
    "# The success and epoch completion for this blurred model test run are then checked,\n",
    "# with corresponding success/failure messages and verification for the saved test model file.\n",
    "if success_test_blurred and epochs_done_test_blurred == EPOCHS_FOR_TEST_RUN:\n",
    "    print(\"SUCCESS: Quick test run for yolo-distance-blured appears to have completed the epoch(s).\")\n",
    "    if os.path.exists(test_best_h5_path_blurred):\n",
    "        print(f\"  Test best model saved at {test_best_h5_path_blurred}\")\n",
    "    else:\n",
    "        print(f\"  Warning: Test best model for blurred was NOT found at {test_best_h5_path_blurred}.\")\n",
    "else:\n",
    "    print(\"FAILED: Quick test run for yolo-distance-blured encountered issues or did not complete epochs. Please check logs above.\")\n",
    "        \n",
    "# 5. Conclusion and Review Instructions:\n",
    "# The cell finally will print a message indicating that the \"Quick Test Run Phase\" is complete.\n",
    "print(\"\\n--- Cell 5: Quick Test Run Phase Complete ---\")\n",
    "# Crucially, Finally some important warnings for my groupmates!\n",
    "print(\">>> IMPORTANT: Review the output and logs from this test phase carefully (check the _test log directories). <<<\")\n",
    "print(\">>> Verify that 'Successfully updated best model...' messages appeared if expected, or that checkpoints were created. <<<\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 5: Quick Test Run Phase ---\n",
      "Quick test run will use 1 epoch(s) per model.\n",
      "\n",
      "--- Starting QUICK TEST RUN for yolo-distance-tuned ---\n",
      "Test logs will go into subdirectories of: /Users/teaguesangster/Code/Python/ComputerVisionFinal/ExistingModel/yolo-with-distance/logs_finetune_tuned_test\n",
      "Test best model will attempt to save to: /Users/teaguesangster/Code/Python/ComputerVisionFinal/runs/tuned/yolo-distance-tuned_QUICK_TEST_best.h5\n",
      "\n",
      "--- Preparing Training Session for: yolo-distance-tuned (Test) ---\n",
      "Will attempt to train for 1 epochs this session.\n",
      "No previous overall best weights found at /Users/teaguesangster/Code/Python/ComputerVisionFinal/runs/tuned/yolo-distance-tuned_QUICK_TEST_best.h5. Loading initial weights: /Users/teaguesangster/Code/Python/ComputerVisionFinal/ExistingModel/yolo-with-distance/weights/trained_final_original.h5\n",
      "NOTE: Online evaluation (EvalCallBack) is disabled for this session to avoid potential errors.\n",
      "Effective training epochs for yolo-distance-tuned (Test): 43 to 43.\n",
      "Clearing Keras session and configuring GPU for yolo-distance-tuned (Test) training...\n",
      "Calling training script for yolo-distance-tuned (Test)...\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Using Yolo3DataGenerator (Sequence) for training data...\n",
      "Using Yolo3DataGenerator (Sequence) for validation data...\n",
      "backbone layers number: 132\n",
      "Create  yolo3_xception model with 9 anchors and 7 classes.\n",
      "model layer number: 199\n",
      "Load weights /Users/teaguesangster/Code/Python/ComputerVisionFinal/ExistingModel/yolo-with-distance/weights/trained_final_original.h5.\n",
      "Freeze the first 196 layers of total 199 layers.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " image_input (InputLayer)    [(None, None, None, 3)]      0         []                            \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)       (None, None, None, 32)       864       ['image_input[0][0]']         \n",
      "                                                                                                  \n",
      " block1_conv1_bn (BatchNorm  (None, None, None, 32)       128       ['block1_conv1[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block1_conv1_act (Activati  (None, None, None, 32)       0         ['block1_conv1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)       18432     ['block1_conv1_act[0][0]']    \n",
      "                                                                                                  \n",
      " block1_conv2_bn (BatchNorm  (None, None, None, 64)       256       ['block1_conv2[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block1_conv2_act (Activati  (None, None, None, 64)       0         ['block1_conv2_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " block2_sepconv1 (Separable  (None, None, None, 128)      8768      ['block1_conv2_act[0][0]']    \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block2_sepconv1_bn (BatchN  (None, None, None, 128)      512       ['block2_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block2_sepconv2_act (Activ  (None, None, None, 128)      0         ['block2_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block2_sepconv2 (Separable  (None, None, None, 128)      17536     ['block2_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block2_sepconv2_bn (BatchN  (None, None, None, 128)      512       ['block2_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, None, None, 128)      8192      ['block1_conv2_act[0][0]']    \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)      0         ['block2_sepconv2_bn[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, None, None, 128)      512       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add (Add)                   (None, None, None, 128)      0         ['block2_pool[0][0]',         \n",
      "                                                                     'batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " block3_sepconv1_act (Activ  (None, None, None, 128)      0         ['add[0][0]']                 \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block3_sepconv1 (Separable  (None, None, None, 256)      33920     ['block3_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block3_sepconv1_bn (BatchN  (None, None, None, 256)      1024      ['block3_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block3_sepconv2_act (Activ  (None, None, None, 256)      0         ['block3_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block3_sepconv2 (Separable  (None, None, None, 256)      67840     ['block3_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block3_sepconv2_bn (BatchN  (None, None, None, 256)      1024      ['block3_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, None, None, 256)      32768     ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)      0         ['block3_sepconv2_bn[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, None, None, 256)      1024      ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, None, None, 256)      0         ['block3_pool[0][0]',         \n",
      "                                                                     'batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block4_sepconv1_act (Activ  (None, None, None, 256)      0         ['add_1[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block4_sepconv1 (Separable  (None, None, None, 728)      188672    ['block4_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block4_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block4_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block4_sepconv2_act (Activ  (None, None, None, 728)      0         ['block4_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block4_sepconv2 (Separable  (None, None, None, 728)      536536    ['block4_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block4_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block4_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, None, None, 728)      186368    ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 728)      0         ['block4_sepconv2_bn[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, None, None, 728)      2912      ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, None, None, 728)      0         ['block4_pool[0][0]',         \n",
      "                                                                     'batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block5_sepconv1_act (Activ  (None, None, None, 728)      0         ['add_2[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block5_sepconv1 (Separable  (None, None, None, 728)      536536    ['block5_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block5_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block5_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block5_sepconv2_act (Activ  (None, None, None, 728)      0         ['block5_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block5_sepconv2 (Separable  (None, None, None, 728)      536536    ['block5_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block5_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block5_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block5_sepconv3_act (Activ  (None, None, None, 728)      0         ['block5_sepconv2_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block5_sepconv3 (Separable  (None, None, None, 728)      536536    ['block5_sepconv3_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block5_sepconv3_bn (BatchN  (None, None, None, 728)      2912      ['block5_sepconv3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, None, None, 728)      0         ['block5_sepconv3_bn[0][0]',  \n",
      "                                                                     'add_2[0][0]']               \n",
      "                                                                                                  \n",
      " block6_sepconv1_act (Activ  (None, None, None, 728)      0         ['add_3[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block6_sepconv1 (Separable  (None, None, None, 728)      536536    ['block6_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block6_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block6_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block6_sepconv2_act (Activ  (None, None, None, 728)      0         ['block6_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block6_sepconv2 (Separable  (None, None, None, 728)      536536    ['block6_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block6_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block6_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block6_sepconv3_act (Activ  (None, None, None, 728)      0         ['block6_sepconv2_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block6_sepconv3 (Separable  (None, None, None, 728)      536536    ['block6_sepconv3_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block6_sepconv3_bn (BatchN  (None, None, None, 728)      2912      ['block6_sepconv3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, None, None, 728)      0         ['block6_sepconv3_bn[0][0]',  \n",
      "                                                                     'add_3[0][0]']               \n",
      "                                                                                                  \n",
      " block7_sepconv1_act (Activ  (None, None, None, 728)      0         ['add_4[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block7_sepconv1 (Separable  (None, None, None, 728)      536536    ['block7_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block7_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block7_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block7_sepconv2_act (Activ  (None, None, None, 728)      0         ['block7_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block7_sepconv2 (Separable  (None, None, None, 728)      536536    ['block7_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block7_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block7_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block7_sepconv3_act (Activ  (None, None, None, 728)      0         ['block7_sepconv2_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block7_sepconv3 (Separable  (None, None, None, 728)      536536    ['block7_sepconv3_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block7_sepconv3_bn (BatchN  (None, None, None, 728)      2912      ['block7_sepconv3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, None, None, 728)      0         ['block7_sepconv3_bn[0][0]',  \n",
      "                                                                     'add_4[0][0]']               \n",
      "                                                                                                  \n",
      " block8_sepconv1_act (Activ  (None, None, None, 728)      0         ['add_5[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block8_sepconv1 (Separable  (None, None, None, 728)      536536    ['block8_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block8_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block8_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block8_sepconv2_act (Activ  (None, None, None, 728)      0         ['block8_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block8_sepconv2 (Separable  (None, None, None, 728)      536536    ['block8_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block8_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block8_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block8_sepconv3_act (Activ  (None, None, None, 728)      0         ['block8_sepconv2_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block8_sepconv3 (Separable  (None, None, None, 728)      536536    ['block8_sepconv3_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block8_sepconv3_bn (BatchN  (None, None, None, 728)      2912      ['block8_sepconv3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, None, None, 728)      0         ['block8_sepconv3_bn[0][0]',  \n",
      "                                                                     'add_5[0][0]']               \n",
      "                                                                                                  \n",
      " block9_sepconv1_act (Activ  (None, None, None, 728)      0         ['add_6[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block9_sepconv1 (Separable  (None, None, None, 728)      536536    ['block9_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block9_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block9_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block9_sepconv2_act (Activ  (None, None, None, 728)      0         ['block9_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block9_sepconv2 (Separable  (None, None, None, 728)      536536    ['block9_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block9_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block9_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block9_sepconv3_act (Activ  (None, None, None, 728)      0         ['block9_sepconv2_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block9_sepconv3 (Separable  (None, None, None, 728)      536536    ['block9_sepconv3_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block9_sepconv3_bn (BatchN  (None, None, None, 728)      2912      ['block9_sepconv3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, None, None, 728)      0         ['block9_sepconv3_bn[0][0]',  \n",
      "                                                                     'add_6[0][0]']               \n",
      "                                                                                                  \n",
      " block10_sepconv1_act (Acti  (None, None, None, 728)      0         ['add_7[0][0]']               \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block10_sepconv1 (Separabl  (None, None, None, 728)      536536    ['block10_sepconv1_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block10_sepconv1_bn (Batch  (None, None, None, 728)      2912      ['block10_sepconv1[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block10_sepconv2_act (Acti  (None, None, None, 728)      0         ['block10_sepconv1_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block10_sepconv2 (Separabl  (None, None, None, 728)      536536    ['block10_sepconv2_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block10_sepconv2_bn (Batch  (None, None, None, 728)      2912      ['block10_sepconv2[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block10_sepconv3_act (Acti  (None, None, None, 728)      0         ['block10_sepconv2_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block10_sepconv3 (Separabl  (None, None, None, 728)      536536    ['block10_sepconv3_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block10_sepconv3_bn (Batch  (None, None, None, 728)      2912      ['block10_sepconv3[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, None, None, 728)      0         ['block10_sepconv3_bn[0][0]', \n",
      "                                                                     'add_7[0][0]']               \n",
      "                                                                                                  \n",
      " block11_sepconv1_act (Acti  (None, None, None, 728)      0         ['add_8[0][0]']               \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block11_sepconv1 (Separabl  (None, None, None, 728)      536536    ['block11_sepconv1_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block11_sepconv1_bn (Batch  (None, None, None, 728)      2912      ['block11_sepconv1[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block11_sepconv2_act (Acti  (None, None, None, 728)      0         ['block11_sepconv1_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block11_sepconv2 (Separabl  (None, None, None, 728)      536536    ['block11_sepconv2_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block11_sepconv2_bn (Batch  (None, None, None, 728)      2912      ['block11_sepconv2[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block11_sepconv3_act (Acti  (None, None, None, 728)      0         ['block11_sepconv2_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block11_sepconv3 (Separabl  (None, None, None, 728)      536536    ['block11_sepconv3_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block11_sepconv3_bn (Batch  (None, None, None, 728)      2912      ['block11_sepconv3[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, None, None, 728)      0         ['block11_sepconv3_bn[0][0]', \n",
      "                                                                     'add_8[0][0]']               \n",
      "                                                                                                  \n",
      " block12_sepconv1_act (Acti  (None, None, None, 728)      0         ['add_9[0][0]']               \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block12_sepconv1 (Separabl  (None, None, None, 728)      536536    ['block12_sepconv1_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block12_sepconv1_bn (Batch  (None, None, None, 728)      2912      ['block12_sepconv1[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block12_sepconv2_act (Acti  (None, None, None, 728)      0         ['block12_sepconv1_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block12_sepconv2 (Separabl  (None, None, None, 728)      536536    ['block12_sepconv2_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block12_sepconv2_bn (Batch  (None, None, None, 728)      2912      ['block12_sepconv2[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block12_sepconv3_act (Acti  (None, None, None, 728)      0         ['block12_sepconv2_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block12_sepconv3 (Separabl  (None, None, None, 728)      536536    ['block12_sepconv3_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block12_sepconv3_bn (Batch  (None, None, None, 728)      2912      ['block12_sepconv3[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, None, None, 728)      0         ['block12_sepconv3_bn[0][0]', \n",
      "                                                                     'add_9[0][0]']               \n",
      "                                                                                                  \n",
      " block13_sepconv1_act (Acti  (None, None, None, 728)      0         ['add_10[0][0]']              \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block13_sepconv1 (Separabl  (None, None, None, 728)      536536    ['block13_sepconv1_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block13_sepconv1_bn (Batch  (None, None, None, 728)      2912      ['block13_sepconv1[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block13_sepconv2_act (Acti  (None, None, None, 728)      0         ['block13_sepconv1_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block13_sepconv2 (Separabl  (None, None, None, 1024)     752024    ['block13_sepconv2_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block13_sepconv2_bn (Batch  (None, None, None, 1024)     4096      ['block13_sepconv2[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, None, None, 1024)     745472    ['add_10[0][0]']              \n",
      "                                                                                                  \n",
      " block13_pool (MaxPooling2D  (None, None, None, 1024)     0         ['block13_sepconv2_bn[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, None, None, 1024)     4096      ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, None, None, 1024)     0         ['block13_pool[0][0]',        \n",
      "                                                                     'batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block14_sepconv1 (Separabl  (None, None, None, 1536)     1582080   ['add_11[0][0]']              \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block14_sepconv1_bn (Batch  (None, None, None, 1536)     6144      ['block14_sepconv1[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block14_sepconv1_act (Acti  (None, None, None, 1536)     0         ['block14_sepconv1_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block14_sepconv2 (Separabl  (None, None, None, 2048)     3159552   ['block14_sepconv1_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block14_sepconv2_bn (Batch  (None, None, None, 2048)     8192      ['block14_sepconv2[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block14_sepconv2_act (Acti  (None, None, None, 2048)     0         ['block14_sepconv2_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, None, None, 512)      1048576   ['block14_sepconv2_act[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, None, None, 512)      2048      ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, None, None, 512)      0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, None, None, 1024)     4718592   ['leaky_re_lu[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, None, None, 1024)     4096      ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, None, None, 1024)     0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, None, None, 512)      524288    ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, None, None, 512)      2048      ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, None, None, 512)      0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, None, None, 1024)     4718592   ['leaky_re_lu_2[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, None, None, 1024)     4096      ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, None, None, 1024)     0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, None, None, 512)      524288    ['leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, None, None, 512)      2048      ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, None, None, 512)      0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, None, None, 256)      131072    ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, None, None, 256)      1024      ['conv2d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, None, None, 256)      0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2  (None, None, None, 256)      0         ['leaky_re_lu_6[0][0]']       \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, None, None, 1280)     0         ['up_sampling2d[0][0]',       \n",
      "                                                                     'block13_sepconv2_bn[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, None, None, 256)      327680    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, None, None, 256)      1024      ['conv2d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, None, None, 256)      0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, None, None, 512)      1179648   ['leaky_re_lu_7[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, None, None, 512)      2048      ['conv2d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, None, None, 512)      0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, None, None, 256)      131072    ['leaky_re_lu_8[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, None, None, 256)      1024      ['conv2d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, None, None, 256)      0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, None, None, 512)      1179648   ['leaky_re_lu_9[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, None, None, 512)      2048      ['conv2d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, None, None, 512)      0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, None, None, 256)      131072    ['leaky_re_lu_10[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, None, None, 256)      1024      ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, None, None, 256)      0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, None, None, 128)      32768     ['leaky_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, None, None, 128)      512       ['conv2d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, None, None, 128)      0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSamplin  (None, None, None, 128)      0         ['leaky_re_lu_13[0][0]']      \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, None, None, 856)      0         ['up_sampling2d_1[0][0]',     \n",
      " )                                                                   'block4_sepconv2_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, None, None, 128)      109568    ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, None, None, 128)      512       ['conv2d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, None, None, 128)      0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, None, None, 256)      294912    ['leaky_re_lu_14[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, None, None, 256)      1024      ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, None, None, 256)      0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, None, None, 128)      32768     ['leaky_re_lu_15[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, None, None, 128)      512       ['conv2d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, None, None, 128)      0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, None, None, 256)      294912    ['leaky_re_lu_16[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, None, None, 256)      1024      ['conv2d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, None, None, 256)      0         ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, None, None, 128)      32768     ['leaky_re_lu_17[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, None, None, 128)      512       ['conv2d_22[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, None, None, 128)      0         ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, None, None, 1024)     4718592   ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, None, None, 512)      1179648   ['leaky_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, None, None, 256)      294912    ['leaky_re_lu_18[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, None, None, 1024)     4096      ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, None, None, 512)      2048      ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, None, None, 256)      1024      ['conv2d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, None, None, 1024)     0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, None, None, 512)      0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, None, None, 256)      0         ['batch_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " predict_conv_1 (Conv2D)     (None, None, None, 39)       39975     ['leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " predict_conv_2 (Conv2D)     (None, None, None, 39)       20007     ['leaky_re_lu_12[0][0]']      \n",
      "                                                                                                  \n",
      " predict_conv_3 (Conv2D)     (None, None, None, 39)       10023     ['leaky_re_lu_19[0][0]']      \n",
      "                                                                                                  \n",
      " y_true_0 (InputLayer)       [(None, None, None, 3, 13)   0         []                            \n",
      "                             ]                                                                    \n",
      "                                                                                                  \n",
      " y_true_1 (InputLayer)       [(None, None, None, 3, 13)   0         []                            \n",
      "                             ]                                                                    \n",
      "                                                                                                  \n",
      " y_true_2 (InputLayer)       [(None, None, None, 3, 13)   0         []                            \n",
      "                             ]                                                                    \n",
      "                                                                                                  \n",
      " yolo_loss (Lambda)          ((1,),                       0         ['predict_conv_1[0][0]',      \n",
      "                              (),                                    'predict_conv_2[0][0]',      \n",
      "                              (),                                    'predict_conv_3[0][0]',      \n",
      "                              (),                                    'y_true_0[0][0]',            \n",
      "                              ())                                    'y_true_1[0][0]',            \n",
      "                                                                     'y_true_2[0][0]']            \n",
      "                                                                                                  \n",
      " add_metric (AddMetric)      ()                           0         ['yolo_loss[0][1]']           \n",
      "                                                                                                  \n",
      " add_metric_1 (AddMetric)    ()                           0         ['yolo_loss[0][2]']           \n",
      "                                                                                                  \n",
      " add_metric_2 (AddMetric)    ()                           0         ['yolo_loss[0][3]']           \n",
      "                                                                                                  \n",
      " add_metric_3 (AddMetric)    ()                           0         ['yolo_loss[0][4]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 42570653 (162.39 MB)\n",
      "Trainable params: 70005 (273.46 KB)\n",
      "Non-trainable params: 42500648 (162.13 MB)\n",
      "__________________________________________________________________________________________________\n",
      "Skipping transfer training stage as transfer_epoch is 0.\n",
      "Unfreeze and continue training (fine-tuning stage).\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " image_input (InputLayer)    [(None, None, None, 3)]      0         []                            \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)       (None, None, None, 32)       864       ['image_input[0][0]']         \n",
      "                                                                                                  \n",
      " block1_conv1_bn (BatchNorm  (None, None, None, 32)       128       ['block1_conv1[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block1_conv1_act (Activati  (None, None, None, 32)       0         ['block1_conv1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)       18432     ['block1_conv1_act[0][0]']    \n",
      "                                                                                                  \n",
      " block1_conv2_bn (BatchNorm  (None, None, None, 64)       256       ['block1_conv2[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block1_conv2_act (Activati  (None, None, None, 64)       0         ['block1_conv2_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " block2_sepconv1 (Separable  (None, None, None, 128)      8768      ['block1_conv2_act[0][0]']    \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block2_sepconv1_bn (BatchN  (None, None, None, 128)      512       ['block2_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block2_sepconv2_act (Activ  (None, None, None, 128)      0         ['block2_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block2_sepconv2 (Separable  (None, None, None, 128)      17536     ['block2_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block2_sepconv2_bn (BatchN  (None, None, None, 128)      512       ['block2_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, None, None, 128)      8192      ['block1_conv2_act[0][0]']    \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)      0         ['block2_sepconv2_bn[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, None, None, 128)      512       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add (Add)                   (None, None, None, 128)      0         ['block2_pool[0][0]',         \n",
      "                                                                     'batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " block3_sepconv1_act (Activ  (None, None, None, 128)      0         ['add[0][0]']                 \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block3_sepconv1 (Separable  (None, None, None, 256)      33920     ['block3_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block3_sepconv1_bn (BatchN  (None, None, None, 256)      1024      ['block3_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block3_sepconv2_act (Activ  (None, None, None, 256)      0         ['block3_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block3_sepconv2 (Separable  (None, None, None, 256)      67840     ['block3_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block3_sepconv2_bn (BatchN  (None, None, None, 256)      1024      ['block3_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, None, None, 256)      32768     ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)      0         ['block3_sepconv2_bn[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, None, None, 256)      1024      ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, None, None, 256)      0         ['block3_pool[0][0]',         \n",
      "                                                                     'batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block4_sepconv1_act (Activ  (None, None, None, 256)      0         ['add_1[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block4_sepconv1 (Separable  (None, None, None, 728)      188672    ['block4_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block4_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block4_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block4_sepconv2_act (Activ  (None, None, None, 728)      0         ['block4_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block4_sepconv2 (Separable  (None, None, None, 728)      536536    ['block4_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block4_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block4_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, None, None, 728)      186368    ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 728)      0         ['block4_sepconv2_bn[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, None, None, 728)      2912      ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, None, None, 728)      0         ['block4_pool[0][0]',         \n",
      "                                                                     'batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block5_sepconv1_act (Activ  (None, None, None, 728)      0         ['add_2[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block5_sepconv1 (Separable  (None, None, None, 728)      536536    ['block5_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block5_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block5_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block5_sepconv2_act (Activ  (None, None, None, 728)      0         ['block5_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block5_sepconv2 (Separable  (None, None, None, 728)      536536    ['block5_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block5_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block5_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block5_sepconv3_act (Activ  (None, None, None, 728)      0         ['block5_sepconv2_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block5_sepconv3 (Separable  (None, None, None, 728)      536536    ['block5_sepconv3_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block5_sepconv3_bn (BatchN  (None, None, None, 728)      2912      ['block5_sepconv3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, None, None, 728)      0         ['block5_sepconv3_bn[0][0]',  \n",
      "                                                                     'add_2[0][0]']               \n",
      "                                                                                                  \n",
      " block6_sepconv1_act (Activ  (None, None, None, 728)      0         ['add_3[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block6_sepconv1 (Separable  (None, None, None, 728)      536536    ['block6_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block6_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block6_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block6_sepconv2_act (Activ  (None, None, None, 728)      0         ['block6_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block6_sepconv2 (Separable  (None, None, None, 728)      536536    ['block6_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block6_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block6_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block6_sepconv3_act (Activ  (None, None, None, 728)      0         ['block6_sepconv2_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block6_sepconv3 (Separable  (None, None, None, 728)      536536    ['block6_sepconv3_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block6_sepconv3_bn (BatchN  (None, None, None, 728)      2912      ['block6_sepconv3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, None, None, 728)      0         ['block6_sepconv3_bn[0][0]',  \n",
      "                                                                     'add_3[0][0]']               \n",
      "                                                                                                  \n",
      " block7_sepconv1_act (Activ  (None, None, None, 728)      0         ['add_4[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block7_sepconv1 (Separable  (None, None, None, 728)      536536    ['block7_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block7_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block7_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block7_sepconv2_act (Activ  (None, None, None, 728)      0         ['block7_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block7_sepconv2 (Separable  (None, None, None, 728)      536536    ['block7_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block7_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block7_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block7_sepconv3_act (Activ  (None, None, None, 728)      0         ['block7_sepconv2_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block7_sepconv3 (Separable  (None, None, None, 728)      536536    ['block7_sepconv3_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block7_sepconv3_bn (BatchN  (None, None, None, 728)      2912      ['block7_sepconv3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, None, None, 728)      0         ['block7_sepconv3_bn[0][0]',  \n",
      "                                                                     'add_4[0][0]']               \n",
      "                                                                                                  \n",
      " block8_sepconv1_act (Activ  (None, None, None, 728)      0         ['add_5[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block8_sepconv1 (Separable  (None, None, None, 728)      536536    ['block8_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block8_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block8_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block8_sepconv2_act (Activ  (None, None, None, 728)      0         ['block8_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block8_sepconv2 (Separable  (None, None, None, 728)      536536    ['block8_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block8_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block8_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block8_sepconv3_act (Activ  (None, None, None, 728)      0         ['block8_sepconv2_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block8_sepconv3 (Separable  (None, None, None, 728)      536536    ['block8_sepconv3_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block8_sepconv3_bn (BatchN  (None, None, None, 728)      2912      ['block8_sepconv3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, None, None, 728)      0         ['block8_sepconv3_bn[0][0]',  \n",
      "                                                                     'add_5[0][0]']               \n",
      "                                                                                                  \n",
      " block9_sepconv1_act (Activ  (None, None, None, 728)      0         ['add_6[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block9_sepconv1 (Separable  (None, None, None, 728)      536536    ['block9_sepconv1_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block9_sepconv1_bn (BatchN  (None, None, None, 728)      2912      ['block9_sepconv1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block9_sepconv2_act (Activ  (None, None, None, 728)      0         ['block9_sepconv1_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block9_sepconv2 (Separable  (None, None, None, 728)      536536    ['block9_sepconv2_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block9_sepconv2_bn (BatchN  (None, None, None, 728)      2912      ['block9_sepconv2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block9_sepconv3_act (Activ  (None, None, None, 728)      0         ['block9_sepconv2_bn[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block9_sepconv3 (Separable  (None, None, None, 728)      536536    ['block9_sepconv3_act[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block9_sepconv3_bn (BatchN  (None, None, None, 728)      2912      ['block9_sepconv3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, None, None, 728)      0         ['block9_sepconv3_bn[0][0]',  \n",
      "                                                                     'add_6[0][0]']               \n",
      "                                                                                                  \n",
      " block10_sepconv1_act (Acti  (None, None, None, 728)      0         ['add_7[0][0]']               \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block10_sepconv1 (Separabl  (None, None, None, 728)      536536    ['block10_sepconv1_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block10_sepconv1_bn (Batch  (None, None, None, 728)      2912      ['block10_sepconv1[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block10_sepconv2_act (Acti  (None, None, None, 728)      0         ['block10_sepconv1_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block10_sepconv2 (Separabl  (None, None, None, 728)      536536    ['block10_sepconv2_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block10_sepconv2_bn (Batch  (None, None, None, 728)      2912      ['block10_sepconv2[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block10_sepconv3_act (Acti  (None, None, None, 728)      0         ['block10_sepconv2_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block10_sepconv3 (Separabl  (None, None, None, 728)      536536    ['block10_sepconv3_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block10_sepconv3_bn (Batch  (None, None, None, 728)      2912      ['block10_sepconv3[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, None, None, 728)      0         ['block10_sepconv3_bn[0][0]', \n",
      "                                                                     'add_7[0][0]']               \n",
      "                                                                                                  \n",
      " block11_sepconv1_act (Acti  (None, None, None, 728)      0         ['add_8[0][0]']               \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block11_sepconv1 (Separabl  (None, None, None, 728)      536536    ['block11_sepconv1_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block11_sepconv1_bn (Batch  (None, None, None, 728)      2912      ['block11_sepconv1[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block11_sepconv2_act (Acti  (None, None, None, 728)      0         ['block11_sepconv1_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block11_sepconv2 (Separabl  (None, None, None, 728)      536536    ['block11_sepconv2_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block11_sepconv2_bn (Batch  (None, None, None, 728)      2912      ['block11_sepconv2[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block11_sepconv3_act (Acti  (None, None, None, 728)      0         ['block11_sepconv2_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block11_sepconv3 (Separabl  (None, None, None, 728)      536536    ['block11_sepconv3_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block11_sepconv3_bn (Batch  (None, None, None, 728)      2912      ['block11_sepconv3[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, None, None, 728)      0         ['block11_sepconv3_bn[0][0]', \n",
      "                                                                     'add_8[0][0]']               \n",
      "                                                                                                  \n",
      " block12_sepconv1_act (Acti  (None, None, None, 728)      0         ['add_9[0][0]']               \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block12_sepconv1 (Separabl  (None, None, None, 728)      536536    ['block12_sepconv1_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block12_sepconv1_bn (Batch  (None, None, None, 728)      2912      ['block12_sepconv1[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block12_sepconv2_act (Acti  (None, None, None, 728)      0         ['block12_sepconv1_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block12_sepconv2 (Separabl  (None, None, None, 728)      536536    ['block12_sepconv2_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block12_sepconv2_bn (Batch  (None, None, None, 728)      2912      ['block12_sepconv2[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block12_sepconv3_act (Acti  (None, None, None, 728)      0         ['block12_sepconv2_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block12_sepconv3 (Separabl  (None, None, None, 728)      536536    ['block12_sepconv3_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block12_sepconv3_bn (Batch  (None, None, None, 728)      2912      ['block12_sepconv3[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, None, None, 728)      0         ['block12_sepconv3_bn[0][0]', \n",
      "                                                                     'add_9[0][0]']               \n",
      "                                                                                                  \n",
      " block13_sepconv1_act (Acti  (None, None, None, 728)      0         ['add_10[0][0]']              \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block13_sepconv1 (Separabl  (None, None, None, 728)      536536    ['block13_sepconv1_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block13_sepconv1_bn (Batch  (None, None, None, 728)      2912      ['block13_sepconv1[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block13_sepconv2_act (Acti  (None, None, None, 728)      0         ['block13_sepconv1_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block13_sepconv2 (Separabl  (None, None, None, 1024)     752024    ['block13_sepconv2_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block13_sepconv2_bn (Batch  (None, None, None, 1024)     4096      ['block13_sepconv2[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, None, None, 1024)     745472    ['add_10[0][0]']              \n",
      "                                                                                                  \n",
      " block13_pool (MaxPooling2D  (None, None, None, 1024)     0         ['block13_sepconv2_bn[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, None, None, 1024)     4096      ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, None, None, 1024)     0         ['block13_pool[0][0]',        \n",
      "                                                                     'batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block14_sepconv1 (Separabl  (None, None, None, 1536)     1582080   ['add_11[0][0]']              \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block14_sepconv1_bn (Batch  (None, None, None, 1536)     6144      ['block14_sepconv1[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block14_sepconv1_act (Acti  (None, None, None, 1536)     0         ['block14_sepconv1_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " block14_sepconv2 (Separabl  (None, None, None, 2048)     3159552   ['block14_sepconv1_act[0][0]']\n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " block14_sepconv2_bn (Batch  (None, None, None, 2048)     8192      ['block14_sepconv2[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block14_sepconv2_act (Acti  (None, None, None, 2048)     0         ['block14_sepconv2_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, None, None, 512)      1048576   ['block14_sepconv2_act[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, None, None, 512)      2048      ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, None, None, 512)      0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, None, None, 1024)     4718592   ['leaky_re_lu[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, None, None, 1024)     4096      ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, None, None, 1024)     0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, None, None, 512)      524288    ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, None, None, 512)      2048      ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, None, None, 512)      0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, None, None, 1024)     4718592   ['leaky_re_lu_2[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, None, None, 1024)     4096      ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, None, None, 1024)     0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, None, None, 512)      524288    ['leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, None, None, 512)      2048      ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, None, None, 512)      0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, None, None, 256)      131072    ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, None, None, 256)      1024      ['conv2d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, None, None, 256)      0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2  (None, None, None, 256)      0         ['leaky_re_lu_6[0][0]']       \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, None, None, 1280)     0         ['up_sampling2d[0][0]',       \n",
      "                                                                     'block13_sepconv2_bn[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, None, None, 256)      327680    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, None, None, 256)      1024      ['conv2d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, None, None, 256)      0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, None, None, 512)      1179648   ['leaky_re_lu_7[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, None, None, 512)      2048      ['conv2d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, None, None, 512)      0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, None, None, 256)      131072    ['leaky_re_lu_8[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, None, None, 256)      1024      ['conv2d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, None, None, 256)      0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, None, None, 512)      1179648   ['leaky_re_lu_9[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, None, None, 512)      2048      ['conv2d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, None, None, 512)      0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, None, None, 256)      131072    ['leaky_re_lu_10[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, None, None, 256)      1024      ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, None, None, 256)      0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, None, None, 128)      32768     ['leaky_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, None, None, 128)      512       ['conv2d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, None, None, 128)      0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSamplin  (None, None, None, 128)      0         ['leaky_re_lu_13[0][0]']      \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, None, None, 856)      0         ['up_sampling2d_1[0][0]',     \n",
      " )                                                                   'block4_sepconv2_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, None, None, 128)      109568    ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, None, None, 128)      512       ['conv2d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, None, None, 128)      0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, None, None, 256)      294912    ['leaky_re_lu_14[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, None, None, 256)      1024      ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, None, None, 256)      0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, None, None, 128)      32768     ['leaky_re_lu_15[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, None, None, 128)      512       ['conv2d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, None, None, 128)      0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, None, None, 256)      294912    ['leaky_re_lu_16[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, None, None, 256)      1024      ['conv2d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, None, None, 256)      0         ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, None, None, 128)      32768     ['leaky_re_lu_17[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, None, None, 128)      512       ['conv2d_22[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, None, None, 128)      0         ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, None, None, 1024)     4718592   ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, None, None, 512)      1179648   ['leaky_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, None, None, 256)      294912    ['leaky_re_lu_18[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, None, None, 1024)     4096      ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, None, None, 512)      2048      ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, None, None, 256)      1024      ['conv2d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, None, None, 1024)     0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, None, None, 512)      0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, None, None, 256)      0         ['batch_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " predict_conv_1 (Conv2D)     (None, None, None, 39)       39975     ['leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " predict_conv_2 (Conv2D)     (None, None, None, 39)       20007     ['leaky_re_lu_12[0][0]']      \n",
      "                                                                                                  \n",
      " predict_conv_3 (Conv2D)     (None, None, None, 39)       10023     ['leaky_re_lu_19[0][0]']      \n",
      "                                                                                                  \n",
      " y_true_0 (InputLayer)       [(None, None, None, 3, 13)   0         []                            \n",
      "                             ]                                                                    \n",
      "                                                                                                  \n",
      " y_true_1 (InputLayer)       [(None, None, None, 3, 13)   0         []                            \n",
      "                             ]                                                                    \n",
      "                                                                                                  \n",
      " y_true_2 (InputLayer)       [(None, None, None, 3, 13)   0         []                            \n",
      "                             ]                                                                    \n",
      "                                                                                                  \n",
      " yolo_loss (Lambda)          ((1,),                       0         ['predict_conv_1[0][0]',      \n",
      "                              (),                                    'predict_conv_2[0][0]',      \n",
      "                              (),                                    'predict_conv_3[0][0]',      \n",
      "                              (),                                    'y_true_0[0][0]',            \n",
      "                              ())                                    'y_true_1[0][0]',            \n",
      "                                                                     'y_true_2[0][0]']            \n",
      "                                                                                                  \n",
      " add_metric (AddMetric)      ()                           0         ['yolo_loss[0][1]']           \n",
      "                                                                                                  \n",
      " add_metric_1 (AddMetric)    ()                           0         ['yolo_loss[0][2]']           \n",
      "                                                                                                  \n",
      " add_metric_2 (AddMetric)    ()                           0         ['yolo_loss[0][3]']           \n",
      "                                                                                                  \n",
      " add_metric_3 (AddMetric)    ()                           0         ['yolo_loss[0][4]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 42570653 (162.39 MB)\n",
      "Trainable params: 42499229 (162.12 MB)\n",
      "Non-trainable params: 71424 (279.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Fine-tune train on 5472 samples, val on 608 samples, with batch size 8, input_shape (608, 608).\n",
      "Epoch 44/44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_SpawnPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process Keras_worker_SpawnPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process Keras_worker_SpawnPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process Keras_worker_SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/site-packages/keras/__init__.py\", line 3, in <module>\n",
      "    from keras import __internal__\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/site-packages/keras/__internal__/__init__.py\", line 3, in <module>\n",
      "    from keras.__internal__ import backend\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/site-packages/keras/__internal__/backend/__init__.py\", line 3, in <module>\n",
      "    from keras.src.backend import _initialize_variables as initialize_variables\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/site-packages/keras/src/__init__.py\", line 21, in <module>\n",
      "    from keras.src import models\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/site-packages/keras/src/models/__init__.py\", line 18, in <module>\n",
      "    from keras.src.engine.functional import Functional\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/site-packages/keras/src/engine/functional.py\", line 23, in <module>\n",
      "    import tensorflow.compat.v2 as tf\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/site-packages/tensorflow/__init__.py\", line 38, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 36, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 26, in <module>\n",
      "    self_check.preload_check()\n",
      "  File \"/opt/anaconda3/envs/keras-yolo3/lib/python3.8/site-packages/tensorflow/python/platform/self_check.py\", line 63, in preload_check\n",
      "    from tensorflow.python.platform import _pywrap_cpu_feature_guard\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <font color=\"dodgerblue\">Cell 6: Time to Train! Main Alternating Training Loop</font>\n",
    "\n",
    "Here we implement the real training loop which will alternate between training the \"tuned\" (normal images) and \"blurred\" (blurred images) YOLO-Distance models. It manages our overall training time, switches between these models, and calls the `run_training_session` function (from Cell 4) for each. The main extra added stuff in this section is for tracking the total epochs trained for each model. <font color=\"chocolate\">Again, because loss was our enemy early on, we had to monitor validation loss improvement.</font> The entire loop is designed to run for a predefined maximum duration (which is defined in Cell 1). <font color=\"chocolate\">We don't have early stopping, as I really just wanted to see how good it could get overnight.</font> We also again have periodic breaks in between the models to alleviate training pressure.\n",
    "\n",
    "**Here's a breakdown of its functionality:**\n",
    "\n",
    "#### <font color=\"teal\">Initialized Variables that control the run:</font>\n",
    "\n",
    "* `best_val_loss_tuned` and `best_val_loss_blurred`: Initialized to infinity to keep track of the best validation loss achieved for each model during this entire run.\n",
    "* `epochs_without_improvement_tuned` and `epochs_without_improvement_blurred`: Initialized to `0`, potentially for early stopping logic if we wanted to add this later on.\n",
    "* `overall_start_time`: Records the starting time of the main loop. (Again, because we only train for a certain duration).\n",
    "* `total_fine_tune_epochs_on_tuned` and `total_fine_tune_epochs_on_blurred`: Counters for the total number of fine-tuning epochs completed for each model *within this specific execution of the notebook*.\n",
    "* `training_active`: A boolean flag set to `True` to control the `while` loop.\n",
    "* `current_model_turn`: A string variable initialized to `\"tuned\"`, indicating which model starts the training sequence.\n",
    "* <font color=\"green\">Prints initial messages</font> about the maximum training duration, epochs per session, and break time (using constants from Cell 1).\n",
    "\n",
    "---\n",
    "\n",
    "#### <font color=\"teal\">Main `while training_active` Loop:</font>\n",
    "\n",
    "This loop continues as long as `training_active` is `True`. <font color=\"chocolate\">This will be set to `False` assuming we went overtime on the last loop.</font>\n",
    "\n",
    "* **Time Check:** At the beginning of each iteration, it checks if the `elapsed_time_seconds` since `overall_start_time` has exceeded `MAX_TOTAL_TRAINING_TIME_SECONDS` (from Cell 1). If so, it prints a message, sets `training_active` to `False`, and breaks out of the loop. This ensures the training doesn't run indefinitely. <font color=\"chocolate\">By default (or at least right now) it's set to 9 hours.</font>\n",
    "* **Status Update:** Prints the current iteration status, including elapsed time and total fine-tune epochs for each model so far in this run.\n",
    "* **Set Up Current Session Parameters:**\n",
    "    Based on the `current_model_turn` (\"tuned\" or \"blurred\"), it sets up session-specific variables:\n",
    "    * `session_model_name_display`: A string for display purposes.\n",
    "    * `session_model_annotation_file`: The path to the correct aggregated annotation file (`annotation_file_tuned` or `annotation_file_blurred`, defined in a previous cell).\n",
    "    * `session_model_base_log_dir`: The correct base log directory (`log_dir_base_tuned` or `log_dir_base_blurred` from Cell 1).\n",
    "    * `session_model_target_best_h5`: The path where the overall best checkpoint for the current model should be saved (`tuned_model_best_h5_path` or `blurred_model_best_h5_path` from Cell 1).\n",
    "    * `session_current_total_fine_tune_epochs`: The current accumulated fine-tuning epochs for the model whose turn it is.\n",
    "* **Run Training Session:**\n",
    "    * Prints which model is about to be trained.\n",
    "    * Calls the `run_training_session` function (defined in Cell 4, which internally calls the `train.py` script) with all the prepared session-specific parameters and `EPOCHS_PER_MAIN_SESSION` (from Cell 1).\n",
    "* **Process Session Results:**\n",
    "    If the session was successful (`success_this_session` is `True`) and at least one epoch was completed:\n",
    "    * It attempts to find the best checkpoint from that specific session using `get_latest_timestamped_log_subdir` and `find_best_epoch_checkpoint_in_session_log_dir` (both from Cell 4).\n",
    "    * If a best checkpoint is found, it tries to parse the `val_loss` from its filename.\n",
    "    * It updates the total fine-tune epochs for the current model.\n",
    "    * It compares the `current_val_loss` with the stored `best_val_loss_` for that model. If the current is better, it updates `best_val_loss_` and resets `epochs_without_improvement_`. Otherwise, it increments `epochs_without_improvement_`. <font color=\"chocolate\">This allows for tracking if the model is still improving.</font>\n",
    "    * If parsing `val_loss` fails or no best checkpoint is found, it still increments the total fine-tune epochs for the model.\n",
    "    * If the session had issues, a warning is printed, and it's noted that the model will be retried on its next turn if time permits.\n",
    "* **Switch Model Turn:** The `current_model_turn` is switched from `\"tuned\"` to `\"blurred\"`, or vice-versa, for the next iteration.\n",
    "* **Post-Session Time Check & Break:**\n",
    "    * Checks the `elapsed_time_seconds` again. If the maximum training time is reached, it exits the loop.\n",
    "    * If `training_active` is still `True`, it prints a message about taking a break (using `BREAK_TIME_SECONDS` from Cell 1) and which model is up next, then pauses using `time.sleep()`. <font color=\"chocolate\">(Again, handling memory pressure).</font>\n",
    "\n",
    "---\n",
    "\n",
    "#### <font color=\"teal\">Loop Termination (`try-except-finally`):</font>\n",
    "\n",
    "* The `while` loop is wrapped in a `try` block.\n",
    "* **`KeyboardInterrupt`:** If the user manually interrupts the training (e.g., Ctrl+C), it catches `KeyboardInterrupt`, prints an interruption message, and sets `training_active` to `False`.\n",
    "* **`finally` Block:** This block executes regardless of how the loop terminated (normally, by time limit, or by interruption).\n",
    "    * It calculates and prints the `final_elapsed_time_seconds`.\n",
    "    * It prints a summary of the total fine-tuning epochs completed for each model during this entire run.\n",
    "    * It indicates the expected path for the final best model for each type and checks if the file exists at that path, printing an appropriate message.\n",
    "    * It reminds the user to check the log directories for detailed TensorBoard logs and session checkpoints.\n",
    "\n",
    "* **Cell Completion Message:** Prints `\"--- Cell 6: Main Training Loop Execution Complete ---\"`."
   ],
   "id": "833e7ffbc5b478d5"
  },
  {
   "cell_type": "code",
   "id": "a604832345c175fd",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Cell 6: Main Alternating Training Loop\n",
    "# Here we implement the real training loop which will alternate between training\n",
    "# the \"tuned\" (normal images) and \"blurred\" (blurred images) YOLO-Distance models.\n",
    "# It manages our overall training time, switches between these models, and calls\n",
    "# the run_training_session function (from Cell 4) for each. The main extra added\n",
    "# stuff in this section is for tracking the total epochs trained for each model.\n",
    "# Again, because loss was our enemy early on, we had to monitor validation loss improvement.\n",
    "# The entire loop is designed to run for a predefined maximum duration (which is defined in cell 1).\n",
    "# We don't have early stopping as I really just wanted to see how good it could get overnight.\n",
    "# We also again have periodic breaks in between the models to alleviate training pressure.\n",
    "print(\"--- Cell 6: Main Alternating Training Loop ---\")\n",
    "\n",
    "# --- Initialized Variables that control the run ---\n",
    "# Add tracking of best validation loss\n",
    "best_val_loss_tuned = float('inf') # Initialized to infinity to keep track of the best validation loss for 'tuned' model.\n",
    "best_val_loss_blurred = float('inf') # Initialized to infinity for 'blurred' model.\n",
    "# Initialized to 0, potentially for early stopping logic if we wanted to add this later on.\n",
    "epochs_without_improvement_tuned = 0\n",
    "epochs_without_improvement_blurred = 0\n",
    "\n",
    "overall_start_time = time.time() # Records the starting time of the main loop. (Again because we only train for a certain duration)\n",
    "# Counters for the total number of fine-tuning epochs completed for each model\n",
    "# within this specific execution of the notebook.\n",
    "total_fine_tune_epochs_on_tuned = 0\n",
    "total_fine_tune_epochs_on_blurred = 0\n",
    "training_active = True # A boolean flag set to True to control the while loop.\n",
    "current_model_turn = \"tuned\"  # A string variable initialized to \"tuned\", indicating which model starts the training sequence.\n",
    "\n",
    "# Prints initial messages about the maximum training duration, epochs per session,\n",
    "# and break time (using constants from Cell 1).\n",
    "print(f\"\\n--- INITIATING MAIN TRAINING LOOP (Max duration: {MAX_TOTAL_TRAINING_TIME_SECONDS / 3600:.2f} hours) ---\")\n",
    "print(f\"Epochs per model session: {EPOCHS_PER_MAIN_SESSION}, Break between sessions: {BREAK_TIME_SECONDS}s\")\n",
    "\n",
    "# --- Loop Termination (try-except-finally) ---\n",
    "# The while loop is wrapped in a try block.\n",
    "try:\n",
    "    # --- Main while training_active Loop ---\n",
    "    # This loop continues as long as training_active is True. This will be set to false\n",
    "    # assuming we went overtime on the last loop.\n",
    "    while training_active:\n",
    "        # Time Check: At the beginning of each iteration, it checks if the elapsed_time_seconds\n",
    "        # since overall_start_time has exceeded MAX_TOTAL_TRAINING_TIME_SECONDS (from Cell 1).\n",
    "        elapsed_time_seconds = time.time() - overall_start_time\n",
    "        if elapsed_time_seconds >= MAX_TOTAL_TRAINING_TIME_SECONDS:\n",
    "            # If so, it prints a message, sets training_active to False, and breaks out of the loop.\n",
    "            # This ensures the training doesn't run indefinitely. By default (Or at least right now) it's set to 9 hours.\n",
    "            print(f\"\\nMaximum training time of {MAX_TOTAL_TRAINING_TIME_SECONDS / 3600:.2f} hours reached. Stopping.\")\n",
    "            training_active = False\n",
    "            break # Exit while loop immediately\n",
    "\n",
    "        # Status Update: Prints the current iteration status, including elapsed time and\n",
    "        # total fine-tune epochs for each model so far in this run.\n",
    "        print(f\"\\n{'='*20} Main Loop Iteration {'='*20}\")\n",
    "        print(f\"Time elapsed: {elapsed_time_seconds / 3600:.2f} / {MAX_TOTAL_TRAINING_TIME_SECONDS / 3600:.2f} hours\")\n",
    "        print(f\"Total fine-tune epochs for Tuned model (this run): {total_fine_tune_epochs_on_tuned}\")\n",
    "        print(f\"Total fine-tune epochs for Blurred model (this run): {total_fine_tune_epochs_on_blurred}\")\n",
    "        \n",
    "        # Initialize session-specific variables\n",
    "        session_model_name_display = \"\"\n",
    "        session_model_annotation_file = \"\"\n",
    "        session_model_base_log_dir = \"\"\n",
    "        session_model_target_best_h5 = \"\"\n",
    "        session_current_total_fine_tune_epochs = 0\n",
    "\n",
    "        # Set Up Current Session Parameters:\n",
    "        # Based on the current_model_turn (\"tuned\" or \"blurred\"), it sets up session-specific variables:\n",
    "        if current_model_turn == \"tuned\":\n",
    "            session_model_name_display = \"yolo-distance-tuned\" # A string for display purposes.\n",
    "            session_model_annotation_file = annotation_file_tuned # The path to the correct aggregated annotation file (annotation_file_tuned, defined in a previous cell).\n",
    "            session_model_base_log_dir = log_dir_base_tuned # The correct base log directory (log_dir_base_tuned from Cell 1).\n",
    "            session_model_target_best_h5 = tuned_model_best_h5_path # The path where the overall best checkpoint for the current model should be saved (tuned_model_best_h5_path from Cell 1).\n",
    "            session_current_total_fine_tune_epochs = total_fine_tune_epochs_on_tuned # The current accumulated fine-tuning epochs for the model whose turn it is.\n",
    "        else: # current_model_turn == \"blurred\"\n",
    "            session_model_name_display = \"yolo-distance-blured\"\n",
    "            session_model_annotation_file = annotation_file_blurred # (annotation_file_blurred, defined in a previous cell).\n",
    "            session_model_base_log_dir = log_dir_base_blurred # (log_dir_base_blurred from Cell 1).\n",
    "            session_model_target_best_h5 = blurred_model_best_h5_path # (blurred_model_best_h5_path from Cell 1).\n",
    "            session_current_total_fine_tune_epochs = total_fine_tune_epochs_on_blurred\n",
    "        \n",
    "        # Run Training Session:\n",
    "        # Prints which model is about to be trained.\n",
    "        print(f\"\\n>>> Current turn: Training {session_model_name_display} <<<\")\n",
    "        \n",
    "        # Calls the run_training_session function (defined in Cell 4, which internally calls\n",
    "        # the train.py script) with all the prepared session-specific parameters and\n",
    "        # EPOCHS_PER_MAIN_SESSION (from Cell 1).\n",
    "        success_this_session, epochs_completed_this_session = run_training_session(\n",
    "            model_name_str=session_model_name_display, # For logging within the function\n",
    "            current_total_fine_tune_epochs_for_model=session_current_total_fine_tune_epochs,\n",
    "            annotation_file_path=session_model_annotation_file,\n",
    "            model_base_log_dir=session_model_base_log_dir, # Base log dir for train.py\n",
    "            target_overall_best_h5_path=session_model_target_best_h5, # Where to copy the session's best\n",
    "            epochs_to_run_this_session=EPOCHS_PER_MAIN_SESSION\n",
    "        )\n",
    "\n",
    "        # Process Session Results:\n",
    "        # If the session was successful (success_this_session is True) and at least one epoch was completed:\n",
    "        if success_this_session and epochs_completed_this_session > 0:\n",
    "            print(f\"Session for {session_model_name_display} completed {epochs_completed_this_session} epochs successfully.\")\n",
    "            \n",
    "            # Get the last validation loss from the session\n",
    "            # It attempts to find the best checkpoint from that specific session using\n",
    "            # get_latest_timestamped_log_subdir and find_best_epoch_checkpoint_in_session_log_dir (both from Cell 4).\n",
    "            session_specific_log_dir = get_latest_timestamped_log_subdir(session_model_base_log_dir)\n",
    "            best_checkpoint = find_best_epoch_checkpoint_in_session_log_dir(session_specific_log_dir)\n",
    "            \n",
    "            # If a best checkpoint is found, it tries to parse the val_loss from its filename.\n",
    "            if best_checkpoint:\n",
    "                checkpoint_basename = os.path.basename(best_checkpoint)\n",
    "                val_loss_parts = [p for p in checkpoint_basename.split('-') if p.startswith('val_loss')]\n",
    "                \n",
    "                if val_loss_parts:\n",
    "                    try:\n",
    "                        current_val_loss = float(val_loss_parts[0].replace('val_loss', ''))\n",
    "                        \n",
    "                        # It updates the total fine-tune epochs for the current model.\n",
    "                        # It compares the current_val_loss with the stored best_val_loss_ for that model.\n",
    "                        # If the current is better, it updates best_val_loss_ and resets epochs_without_improvement_.\n",
    "                        # Otherwise, it increments epochs_without_improvement_. This allows for tracking if the model is still improving.\n",
    "                        if current_model_turn == \"tuned\":\n",
    "                            total_fine_tune_epochs_on_tuned += epochs_completed_this_session\n",
    "                            if current_val_loss < best_val_loss_tuned:\n",
    "                                print(f\"New best val_loss for tuned model: {current_val_loss:.4f} (previous: {best_val_loss_tuned:.4f})\")\n",
    "                                best_val_loss_tuned = current_val_loss\n",
    "                                epochs_without_improvement_tuned = 0\n",
    "                            else:\n",
    "                                epochs_without_improvement_tuned += epochs_completed_this_session\n",
    "                                print(f\"No improvement for tuned model. Epochs without improvement: {epochs_without_improvement_tuned}\")\n",
    "                        else: # blurred\n",
    "                            total_fine_tune_epochs_on_blurred += epochs_completed_this_session\n",
    "                            if current_val_loss < best_val_loss_blurred:\n",
    "                                print(f\"New best val_loss for blurred model: {current_val_loss:.4f} (previous: {best_val_loss_blurred:.4f})\")\n",
    "                                best_val_loss_blurred = current_val_loss\n",
    "                                epochs_without_improvement_blurred = 0\n",
    "                            else:\n",
    "                                epochs_without_improvement_blurred += epochs_completed_this_session\n",
    "                                print(f\"No improvement for blurred model. Epochs without improvement: {epochs_without_improvement_blurred}\")\n",
    "                    except ValueError:\n",
    "                        print(f\"Could not parse validation loss from checkpoint name: {checkpoint_basename}\")\n",
    "                        # If parsing val_loss fails or no best checkpoint is found,\n",
    "                        # it still increments the total fine-tune epochs for the model.\n",
    "                        if current_model_turn == \"tuned\":\n",
    "                            total_fine_tune_epochs_on_tuned += epochs_completed_this_session\n",
    "                        else: # blurred\n",
    "                            total_fine_tune_epochs_on_blurred += epochs_completed_this_session\n",
    "            else: # No best_checkpoint found this session\n",
    "                if current_model_turn == \"tuned\":\n",
    "                    total_fine_tune_epochs_on_tuned += epochs_completed_this_session\n",
    "                else: # blurred\n",
    "                    total_fine_tune_epochs_on_blurred += epochs_completed_this_session\n",
    "        else: # Session was not successful or no epochs completed\n",
    "            # If the session had issues, a warning is printed, and it's noted that\n",
    "            # the model will be retried on its next turn if time permits.\n",
    "            print(f\"WARNING: Session for {session_model_name_display} had issues or completed 0 epochs. It will be retried on its next turn if time permits.\")\n",
    "\n",
    "        # Switch Model Turn: The current_model_turn is switched from \"tuned\" to \"blurred\",\n",
    "        # or vice-versa, for the next iteration.\n",
    "        if current_model_turn == \"tuned\":\n",
    "            current_model_turn = \"blurred\"\n",
    "        else:\n",
    "            current_model_turn = \"tuned\"\n",
    "\n",
    "        # Post-Session Time Check & Break:\n",
    "        # Checks the elapsed_time_seconds again. If the maximum training time is reached, it exits the loop.\n",
    "        elapsed_time_seconds = time.time() - overall_start_time\n",
    "        if elapsed_time_seconds >= MAX_TOTAL_TRAINING_TIME_SECONDS:\n",
    "            print(f\"\\nMaximum training time reached after session for {session_model_name_display}. Exiting loop.\")\n",
    "            training_active = False # Ensure loop terminates\n",
    "            break # Exit while loop immediately\n",
    "            \n",
    "        # If training_active is still True, it prints a message about taking a break\n",
    "        # (using BREAK_TIME_SECONDS from Cell 1) and which model is up next, then pauses\n",
    "        # using time.sleep(). (Again handling memory pressure)\n",
    "        if training_active: # Only take a break if we are not about to exit due to time limit\n",
    "            print(f\"\\nTaking a {BREAK_TIME_SECONDS} second break (approx {BREAK_TIME_SECONDS/60:.1f} mins)... Next up: {current_model_turn}\")\n",
    "            time.sleep(BREAK_TIME_SECONDS)\n",
    "\n",
    "# KeyboardInterrupt: If the user manually interrupts the training (e.g., Ctrl+C),\n",
    "# it catches KeyboardInterrupt, prints an interruption message, and sets training_active to False.\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n--- Training Interrupted by User (KeyboardInterrupt) ---\")\n",
    "    training_active = False # Ensure loop terminates if interrupted\n",
    "# finally Block: This block executes regardless of how the loop terminated\n",
    "# (normally, by time limit, or by interruption).\n",
    "finally:\n",
    "    final_elapsed_time_seconds = time.time() - overall_start_time\n",
    "    # It calculates and prints the final_elapsed_time_seconds.\n",
    "    print(f\"\\n{'='*20} MAIN TRAINING LOOP FINISHED {'='*20}\")\n",
    "    print(f\"Total script duration: {final_elapsed_time_seconds / 3600:.2f} hours.\")\n",
    "    # It prints a summary of the total fine-tuning epochs completed for each model during this entire run.\n",
    "    print(f\"Total fine-tuning epochs (this run) for yolo-distance-tuned: {total_fine_tune_epochs_on_tuned}\")\n",
    "    # It indicates the expected path for the final best model for each type and checks\n",
    "    # if the file exists at that path, printing an appropriate message.\n",
    "    if os.path.exists(tuned_model_best_h5_path):\n",
    "        print(f\"  Final best model for yolo-distance-tuned is expected at: {tuned_model_best_h5_path}\")\n",
    "    else:\n",
    "        print(f\"  No final best model found for yolo-distance-tuned at the end of the run.\")\n",
    "        \n",
    "    print(f\"Total fine-tuning epochs (this run) for yolo-distance-blured: {total_fine_tune_epochs_on_blurred}\")\n",
    "    if os.path.exists(blurred_model_best_h5_path):\n",
    "        print(f\"  Final best model for yolo-distance-blured is expected at: {blurred_model_best_h5_path}\")\n",
    "    else:\n",
    "        print(f\"  No final best model found for yolo-distance-blured at the end of the run.\")\n",
    "    # It reminds the user to check the log directories for detailed TensorBoard logs and session checkpoints.\n",
    "    print(\"Please check the respective log directories for detailed TensorBoard logs and checkpoints from each session.\")\n",
    "\n",
    "# Cell Completion Message: Prints \"--- Cell 6: Main Training Loop Execution Complete ---\".\n",
    "print(\"--- Cell 6: Main Training Loop Execution Complete ---\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1be325fd8b044ae4",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
